{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![2017fiedler](http://www.engg.ksu.edu/images/2017fiedler.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-State Honor Code\n",
    ">### \"On my honor, as a student, I have neither given nor received unauthorized aid on this academic work.\"\n",
    ">### \"The assignment I am submitting contains my own words without borrowing the words of other people from the Internet or other sources (e.g., articles, lecture notes).”\n",
    ">### Derek W. Christensen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Vector Space Model: Implementation\n",
    "Created on Wed Dec 13 09:50:32 2017\n",
    "@author: Derek Christensen\n",
    ">### Vector Space Model, TF-IDF, Query, Ranked List\n",
    ">### HW 2\n",
    ">### CS 833 Information Retrieval and Text Mining\n",
    ">### Cornelia Caragea\n",
    ">### Department of Computer Science\n",
    ">### Kansas State University\n",
    ">### Fall 2017"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Your task is to implement a basic vector space retrieval system. You will use the Cranfield collection to develop and test your system.\n",
    "\n",
    "#### The Cranfield collection is a standard IR text collection, consisting of 1400 documents from the aerodynamics field, in SGML format. The dataset, a list of queries and relevance judgments associated with these queries are available from Online K-State.\n",
    "\n",
    "#### Tasks: To complete this assignment, you need to use the pre-processing tools implemented during assignment 1.\n",
    "#### Note that you also need to eliminate the SGML tags (e.g., '<'TITLE>', '<'DOC>, '<'TEXT>, etc.) - you should only keep the actual title and text.\n",
    "\n",
    "1. Implement an indexing scheme based on the vector space model, as discussed in class. The\n",
    "steps pointed out in class can be used as guidelines for the implementation. For the weighting\n",
    "scheme, use and experiment with:\n",
    "\n",
    "    • TF-IDF (do not divide TF by the maximum term frequency in a document).<br>\n",
    "    <br>\n",
    "\n",
    "2. For each of the ten queries in the queries.txt file, determine a ranked list of documents, in\n",
    "descending order of their similarity with the query. The output of your retrieval should be\n",
    "a list of (query_id, document_id) pairs.\n",
    "\n",
    "Determine the average precision and recall for the ten queries, when you use:\n",
    "\n",
    "    • top 10 documents in the ranking  \n",
    "    • top 50 documents in the ranking  \n",
    "    • top 100 documents in the ranking  \n",
    "    • top 500 documents in the ranking  \n",
    "\n",
    "Note: A list of relevant documents for each query is provided to you, so that you can determine\n",
    "precision and recall.\n",
    "\n",
    "Submission instructions:\n",
    "\n",
    "1. write a README file including:<br>\n",
    "    • a detailed note about the functionality of each of the above programs<br>\n",
    "    • complete instructions on how to run them<br>\n",
    "    • answers to the questions above<br>\n",
    "\n",
    "2. make sure you include your name in each program and in the README file.\n",
    "3. make sure all your programs run correctly on the CS machines. You will lose 40 points\n",
    "if your code is not running on these machines. The path to the data should be an input\n",
    "parameter, and not hardcoded.\n",
    "4. submit your assignment through Online K-State.\n",
    "\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vector Space Model: Implementation Steps<br>\n",
    "  \n",
    ">### Step 1: Preprocessing\n",
    ">### Step 2: Indexing\n",
    ">### Step 3: Retrival  \n",
    ">### Step 4: Ranking  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pretty Display of Variables"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "https://www.dataquest.io/blog/jupyter-notebook-tips-tricks-shortcuts/\n",
    "\n",
    "The first part of this is pretty widely known. By finishing a Jupyter cell with the name of a variable or unassigned output of a statement, Jupyter will display that variable without the need for a print statement. This is especially useful when dealing with Pandas DataFrames, as the output is neatly formatted into a table.\n",
    "\n",
    "What is known less, is that you can alter a modify the ast_note_interactivity kernel option to make jupyter do this for any variable or statement on it's own line, so you can see the value of multiple statements at once.\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "If you want to set this behaviour for all instances of Jupyter (Notebook and Console), simply create a file ~/.ipython/profile_default/ipython_config.py with the lines below.\n",
    "\n",
    "c = get_config()\n",
    "\n",
    "# Run all nodes interactively\n",
    "c.InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Most performant - Python 2.7 and 3, dict comprehension:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imagine that you have:\n",
    "##### >>> keys = ('name', 'age', 'food')\n",
    "##### >>> values = ('Monty', 42, 'spam')\n",
    "### What is the simplest way to produce the following dictionary ?\n",
    "##### >>> dict = {'name' : 'Monty', 'age' : 42, 'food' : 'spam'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A possible improvement on using the dict constructor is to use the native syntax of a dict comprehension (not a list comprehension, as others have mistakenly put it):\n",
    "\n",
    "##### >>> new_dict = {k: v for k, v in zip(keys, values)}\n",
    "\n",
    "### In _Python 2, zip returns a list,_ *to avoid creating an unnecessary list, use izip instead*\n",
    "(aliased to zip can reduce code changes when you move to Python 3).\n",
    "\n",
    "## >>> from itertools import izip as zip\n",
    " \n",
    "So that is still:\n",
    "\n",
    "## >>> new_dict = {k: v for k, v in zip(keys, values)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Python 2, ideal for <= 2.6\n",
    " \n",
    "izip from itertools becomes zip in Python 3. izip is better than zip for Python 2 (because it avoids the unnecessary list creation), and ideal for 2.6 or below:\n",
    "##### >>> from itertools import izip\n",
    "##### >>> new_dict = dict(izip(keys, values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Python 3\n",
    "In Python 3, zip becomes the same function that was in the itertools module, so that is simply:\n",
    "##### >>> new_dict = dict(zip(keys, values))\n",
    "A dict comprehension would be more performant though (see performance review at the end of this answer)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Result for all cases:\n",
    "In all cases:\n",
    "##### >>> new_dict\n",
    "{'age': 42, 'name': 'Monty', 'food': 'spam'}<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explanation:\n",
    "If we look at the help on dict we see that it takes a variety of forms of arguments:\n",
    "##### >>> help(dict)\n",
    "class dict(object)<br>\n",
    " |  dict() -> new empty dictionary<br>\n",
    " |  dict(mapping) -> new dictionary initialized from a mapping object's<br>\n",
    " |      (key, value) pairs<br>\n",
    " |  dict(iterable) -> new dictionary initialized as if via:<br>\n",
    " |      d = {}<br>\n",
    " |      for k, v in iterable:<br>\n",
    " |          d[k] = v<br>\n",
    " |  dict(**kwargs) -> new dictionary initialized with the name=value pairs<br>\n",
    " |      in the keyword argument list.  For example:  dict(one=1, two=2)<br>\n",
    " \n",
    "The optimal approach is to use an iterable while avoiding creating unnecessary data structures. In Python 2, zip creates an unnecessary list:\n",
    "\n",
    "##### >>> zip(keys, values)\n",
    "[('name', 'Monty'), ('age', 42), ('food', 'spam')]<br>\n",
    " \n",
    "In Python 3, the equivalent would be:\n",
    "##### >>> list(zip(keys, values))\n",
    "[('name', 'Monty'), ('age', 42), ('food', 'spam')]<br>\n",
    " \n",
    "and Python 3's zip merely creates an iterable object:\n",
    "##### >>> zip(keys, values)\n",
    "<zip object at 0x7f0e2ad029c8><br>\n",
    " \n",
    "Since we want to avoid creating unnecessary data structures, we usually want to avoid Python 2's zip (since it creates an unnecessary list)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Less performant alternatives:\n",
    "This is a generator expression being passed to the dict constructor:\n",
    "##### >>> generator_expression = ((k, v) for k, v in zip(keys, values))\n",
    "##### >>> dict(generator_expression)\n",
    "or equivalently:\n",
    "##### >>> dict((k, v) for k, v in zip(keys, values))\n",
    "And this is a list comprehension being passed to the dict constructor:\n",
    "##### >>> dict([(k, v) for k, v in zip(keys, values)])\n",
    "In the first two cases, an extra layer of non-operative (thus unnecessary) computation is placed over the zip iterable, and in the case of the list comprehension, an extra list is unnecessarily created. I would expect all of them to be less performant, and certainly not more-so."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Performance review:\n",
    "In 64 bit Python 3.4.3, on Ubuntu 14.04, ordered from fastest to slowest:\n",
    "##### >>> min(timeit.repeat(lambda: {k: v for k, v in zip(keys, values)}))\n",
    "0.7836067057214677<br>\n",
    "##### >>> min(timeit.repeat(lambda: dict(zip(keys, values))))\n",
    "1.0321204089559615<br>\n",
    "##### >>> min(timeit.repeat(lambda: {keys[i]: values[i] for i in range(len(keys))}))\n",
    "1.0714934510178864<br>\n",
    "##### >>> min(timeit.repeat(lambda: dict([(k, v) for k, v in zip(keys, values)])))\n",
    "1.6110592018812895<br>\n",
    "##### >>> min(timeit.repeat(lambda: dict((k, v) for k, v in zip(keys, values))))\n",
    "1.7361853648908436<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from __future__ import absolute_import, division, print_function\n",
    "# coding: utf-8\n",
    "\n",
    "__author__ = 'Derek W. Christensen'\n",
    "__email__ = 'cderekw@gmail.com'\n",
    "__version__ = '0.0.0'\n",
    "\n",
    "import sys\n",
    "import functools\n",
    "import math\n",
    "import string\n",
    "import random\n",
    "from random import randrange\n",
    "import pprint\n",
    "import subprocess\n",
    "import itertools\n",
    "import hashlib\n",
    "\n",
    "from math import pi\n",
    "from bisect import bisect_left  \n",
    "\n",
    "# Regular Expression\n",
    "import re\n",
    "\n",
    "import gc\n",
    "\n",
    "from array import array\n",
    "\n",
    "import os\n",
    "from os import path\n",
    "\n",
    "from operator import itemgetter, attrgetter\n",
    "\n",
    "# In Python 2, zip returns a list, to avoid creating an unnecessary list,\\\n",
    "# use izip instead\n",
    "# (aliased to zip can reduce code changes when you move to Python 3)\n",
    "from itertools import izip as zip\n",
    "\n",
    "# import timeit\n",
    "# timeit.timeit('x=(1,2,3,4,5,6,7,8,9)', number=100000)\n",
    "\n",
    "# to read and/or save text files or csv files, import csv\n",
    "import csv\n",
    "\n",
    "import collections\n",
    "# Count words in list\n",
    "from collections import Counter\n",
    "from collections import defaultdict\n",
    "\n",
    "# import nltk, which is a python package for natural language processing\n",
    "import nltk\n",
    "# to remove stropwords\n",
    "from nltk.corpus import stopwords\n",
    "# FreqDist, word_tokenize\n",
    "from nltk import FreqDist, sent_tokenize, word_tokenize\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# from nltk.stem.snowball import EnglishStemmer\n",
    "# Assuming we're working with English\n",
    "\n",
    "# Data Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "# import ipython\n",
    "# % matplotlib inline\n",
    "get_ipython().magic(u'matplotlib inline')\n",
    "\n",
    "# python package for text classification\n",
    "import sklearn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "#initialize countvectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "#initialize TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "# support vector machine (another algorithm for classification)\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Evaluating model performance\n",
    "from sklearn import metrics\n",
    "\n",
    "# Excel-like format\n",
    "import pandas as pd\n",
    "# to diplay max rows\n",
    "pd.set_option('display.max_rows', 1000)\n",
    "# to diplay max cols\n",
    "pd.set_option('display.max_columns', 1000)\n",
    "# to define width of cells\n",
    "pd.set_option('display.max_colwidth', 1000)\n",
    "\n",
    "# package for numbers\n",
    "import numpy as np\n",
    "\n",
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "#v1=doc_vec(doc1)\n",
    "#v2=doc_vec(doc2)\n",
    "#print \"Similarity: %s\" % float(dot(v1,v2) / (norm(v1) * norm(v2)))\n",
    "\n",
    "from numpy import zeros\n",
    "#def doc_vec(doc):\n",
    "# v=zeros(len(key_idx)) # returns array([0,0,0....len(key_idx)])\n",
    "\n",
    "\n",
    "# WordCloud\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "\n",
    "# Count words in list\n",
    "# from collections import Counter --> (see above)\n",
    "\n",
    "# Pattern\n",
    "from pattern.en import sentiment\n",
    "\n",
    "# Seaborn\n",
    "import seaborn\n",
    "\n",
    "# IPyton Display\n",
    "from IPython.display import display, HTML\n",
    "# You can include Youtube video in Ipython notebook\n",
    "from IPython.display import YouTubeVideo\n",
    "# Include images in ipython notebook\n",
    "from IPython.display import Image\n",
    "# Include webpages in ipython notebook\n",
    "from IPython.core.display import HTML \n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# #Adding the following lines to a new script will clear all variables each time you rerun the script:\n",
    "# from IPython import get_ipython\n",
    "# get_ipython().magic('reset -sf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# ##########################\n",
    "# Step 1: Preprocessing Functions\n",
    "# ##########################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def getStopwords(dir_path_stopwords)\n",
    "# return(stopwords_from_file)\n",
    "\n",
    "\n",
    "def getStopwords(dir_path_stopwords):\n",
    "    \n",
    "    files_stopwords = os.listdir(dir_path_stopwords)\n",
    "    print('files_stopwords = ', files_stopwords)\n",
    "        \n",
    "    for fsw in files_stopwords:\n",
    "        with open(dir_path_stopwords+'/'+os.path.basename(fsw), 'r') as swfile:\n",
    "            stopwords_from_file = set(swfile.read().splitlines())\n",
    "            \n",
    "#             stopwords_from_file = set(swfile.readlines())\n",
    "            \n",
    "#             stopwords_from_file = [lambda x for x: set(swfile.readlines()).strip()]\n",
    "\n",
    "    print('stopwords_from_file = ', stopwords_from_file)\n",
    "    \n",
    "    return(stopwords_from_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def getFiles(dir_path)\n",
    "# return(files, file_names, file_idx, file_zip, file_dict, file_dict_enum)\n",
    "\n",
    "\n",
    "def getFiles(dir_path):\n",
    "    \n",
    "#     print()\n",
    "#     print('-----BEGIN getFiles METHOD-----')\n",
    "#     print()\n",
    "\n",
    "    files = os.listdir(dir_path)\n",
    "    file_names = os.listdir(dir_path)\n",
    "    print('files = ', files)\n",
    "    print('file_names = ', file_names)\n",
    "    print('len(file_names) = ', len(file_names))\n",
    "    print()\n",
    "    \n",
    "    for i in range(len(file_names)):\n",
    "        file_idx.append(i+1)\n",
    "        print('file_idx[i] ', i, '= ', file_idx[i])\n",
    "    print()\n",
    "    \n",
    "    print('file_idx = ', file_idx)\n",
    "    print()\n",
    "    \n",
    "    file_zip = zip(file_idx,file_names)\n",
    "    print('file_zip = ', file_zip)\n",
    "    print()\n",
    "    \n",
    "    file_dict = dict(file_zip)\n",
    "    print('file_dict = ', file_dict)\n",
    "    print()\n",
    "    \n",
    "    files_dict_enum = {key:value for key, value in enumerate(file_names)}\n",
    "    print('files_dict_enum = ', files_dict_enum)\n",
    "    print()\n",
    "    \n",
    "#     print()\n",
    "#     print('-----END getFiles METHOD-----')\n",
    "#     print()\n",
    "    \n",
    "    return(files, file_names, file_idx, file_zip, file_dict, files_dict_enum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def getLines(files, dir_path)\n",
    "# return(review, docnum, titles, texts)\n",
    "\n",
    "\n",
    "def getLines(files, dir_path):\n",
    "    \n",
    "    print()\n",
    "    print('-----BEGIN getLines METHOD-----')\n",
    "    print()\n",
    "    \n",
    "    # tokenize the words based on white space, removes the punctuation\n",
    "    strtemp = \"\"\n",
    "\n",
    "    for f in files:\n",
    "        with open(dir_path+'/'+os.path.basename(f), 'r') as ipfile:\n",
    "            i = 0\n",
    "            for line in ipfile:\n",
    "                line = line.strip()\n",
    "                if i == 2:\n",
    "                    docnum.append(line)\n",
    "                    review.append(line)\n",
    "                    i += 1\n",
    "                elif i == 5:\n",
    "                    strtemp += line\n",
    "                    strtemp += \" \"\n",
    "                    review.append(line)\n",
    "                    i += 1\n",
    "                    while line != '</TITLE>':\n",
    "                        for line in ipfile:\n",
    "                            line = line.strip()\n",
    "                            if line == '</TITLE>':\n",
    "                                review.append(line)\n",
    "                                i += 1\n",
    "                            else:\n",
    "                                strtemp += line\n",
    "                                strtemp += \" \"\n",
    "                                review.append(line)\n",
    "                                i += 1\n",
    "                            break\n",
    "                    titles.append(strtemp)\n",
    "                    strtemp = \"\"\n",
    "                elif line == '<TEXT>':\n",
    "                    review.append(line)\n",
    "                    i += 1\n",
    "                    while line != '</TEXT>':\n",
    "                        for line in ipfile:\n",
    "                            line = line.strip()\n",
    "                            if line == '</TEXT>':\n",
    "                                review.append(line)\n",
    "                                i += 1\n",
    "                            else:\n",
    "                                strtemp += line\n",
    "                                strtemp += \" \"\n",
    "                                review.append(line)\n",
    "                                i += 1\n",
    "                            break\n",
    "                    texts.append(strtemp)\n",
    "                    strtemp = \"\"\n",
    "                else:\n",
    "                    review.append(line)\n",
    "                    i += 1\n",
    "#            print('\\nDone with file = ', ipfile, '\\n')\n",
    "#            print()\n",
    "\n",
    "    print()\n",
    "    print('-----END getLines METHOD-----')\n",
    "    print()\n",
    "\n",
    "    return(review, docnum, titles, texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def getPerDocCorp(titles, texts)\n",
    "# return(perDocCorp, corpus)\n",
    "\n",
    "\n",
    "def getPerDocCorp(titles, texts):\n",
    "    \n",
    "    print()\n",
    "    print('-----BEGIN getPerDocCorp METHOD-----')\n",
    "    print()\n",
    "    \n",
    "    strtemp = \"\"\n",
    "    corpustemp = \"\"\n",
    "    \n",
    "    for i in range(len(titles)):\n",
    "        strtemp += titles[i]\n",
    "        strtemp += texts[i]\n",
    "        print('\\ni = ', i)\n",
    "        print('strtemp = ', strtemp)\n",
    "        print()\n",
    "        corpustemp += strtemp\n",
    "        print('corpustemp = ', corpustemp, '\\n')\n",
    "        perDocCorp.append(strtemp)\n",
    "        strtemp = \"\"\n",
    "    \n",
    "    corpus.append(corpustemp)\n",
    "    print('corpus = ', corpus)\n",
    "    \n",
    "    print()\n",
    "    print('-----END getPerDocCorp METHOD-----')\n",
    "    print()\n",
    "    \n",
    "    return(perDocCorp, corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def getPerDocCorpClean(perDocCorp)\n",
    "# return(perDocCorpClean, perDocLen, fdistPerDoc, fdistPerDocLen,\n",
    "#       freq_word_PerDoc)\n",
    "\n",
    "# for ea perDocCorp: tokenize, clean, stem, lem, stopwords, \\\n",
    "# shortwords, etc.\n",
    "\n",
    "\n",
    "def getPerDocCorpClean(perDocCorp):\n",
    "\n",
    "    print()\n",
    "    print('-----BEGIN getPerDocCorpClean METHOD-----')\n",
    "    print()\n",
    "    \n",
    "    i=0\n",
    "    for doc in perDocCorp:\n",
    "        \n",
    "        # lenDocTokens = 0\n",
    "        # fdist = {}\n",
    "        # lenDocFdist = 0\n",
    "\n",
    "        tokens = str(doc)\n",
    "        print('tokens = str(doc)')\n",
    "        print(len(tokens))\n",
    "        print('\\ntokens [', i, '] = ', tokens, '\\n')\n",
    "\n",
    "        # lowecases for content analytics ... we assume, for example, \\\n",
    "        # LOVE is sames love\n",
    "        tokens = tokens.lower()\n",
    "        print('tokens = tokens.lower()')\n",
    "        print(len(tokens))\n",
    "        print('\\ntokens [', i, '] = ', tokens, '\\n')\n",
    "\n",
    "        # the dataset contains useless characters and numbers\n",
    "        # Remove useless numbers and alphanumerical words\n",
    "        # use regular expression ... a-zA-Z0-9 refers to all English \\\n",
    "        # characters (lowercase & uppercase) and numbers\n",
    "        # ^a-zA-Z0-9 is opposite of a-zA-Z0-9\n",
    "        tokens = re.sub(\"[^a-zA-Z0-9]\", \" \", tokens)\n",
    "        print('tokens = re.sub(\"[^a-zA-Z0-9]\", \" \", tokens)')\n",
    "        print(len(tokens))\n",
    "        print('\\ntokens [', i, '] = ', tokens, '\\n')\n",
    "\n",
    "        # tokenization or word split\n",
    "        tokens = word_tokenize(tokens)\n",
    "        print('tokens = word_tokenize(tokens)')\n",
    "        print(len(tokens))\n",
    "        print('\\ntokens [', i, '] = ', tokens, '\\n')\n",
    "\n",
    "        # Filter non-alphanumeric characters from tokens\n",
    "        tokens = [word for word in tokens if word.isalpha()]\n",
    "\n",
    "        # remove short words\n",
    "        tokens = [word for word in tokens if len(word) > 2]\n",
    "\n",
    "        # remove common words\n",
    "        stoplist = stopwords.words('english')\n",
    "        # if you want to remove additional words EXAMPLE\n",
    "        # more = set(['much', 'even', 'time', 'story'])\n",
    "        # more = set(['the'])\n",
    "        # stoplist = set(stoplist) | more\n",
    "        stoplist = set(stoplist) | stopwords_from_file\n",
    "        stoplist = set(stoplist)\n",
    "        tokens = [word for word in tokens if word not in stoplist]\n",
    "\n",
    "        # stemming\n",
    "        stemmer = PorterStemmer()\n",
    "        tokens = [stemmer.stem(word) for word in tokens]\n",
    "\n",
    "# -----CLEANING COMPLETE-----\n",
    "\n",
    "        perDocCorpClean.append(tokens)\n",
    "        print('\\nperDocCorpClean[', i, '] = ', tokens, '\\n')\n",
    "\n",
    "        lenDocTokens = len(tokens)\n",
    "        print(lenDocTokens)\n",
    "        perDocLen.append(lenDocTokens)\n",
    "        print(perDocLen[i])\n",
    "        print('\\nperDocLen[', i, '] = ', perDocLen[i], '\\n')\n",
    "\n",
    "        fdist = nltk.FreqDist(tokens)\n",
    "        fdist\n",
    "        fdistPerDoc.append(fdist)\n",
    "        print(fdistPerDoc[i])\n",
    "        print('\\nfdistPerDoc[', i, '] = ', fdistPerDoc[i], '\\n')\n",
    "        print('\\nfdistPerDoc[i].most_common(10) = \\n',\n",
    "              fdistPerDoc[i].most_common(10), '\\n')\n",
    "\n",
    "        lenDocFdist = len(fdist)\n",
    "        print(lenDocFdist)\n",
    "        fdistPerDocLen.append(lenDocFdist)\n",
    "        print(fdistPerDocLen[i])\n",
    "        print('\\nfdistPerDocLen[', i, '] = ', fdistPerDocLen[i], '\\n')\n",
    "\n",
    "        # freq_word_PerDoc = []\n",
    "        # prepare the results of word frequency on corpus data as a list\n",
    "\n",
    "        freq_word = []\n",
    "\n",
    "        # two values or columns in fdist_a\n",
    "        print()\n",
    "        j = 0\n",
    "        for k, v in fdist.items():\n",
    "            freq_word.append([k, v])\n",
    "            print('freq_word[', j, '] = ', freq_word[j])\n",
    "            j += 1\n",
    "\n",
    "        # make it like an Excel worksheet\n",
    "        wordlist = pd.DataFrame(freq_word)\n",
    "\n",
    "        # pd.set_option('display.max_rows', 1000)\n",
    "        pd.set_option('display.max_rows', 10)\n",
    "        \n",
    "        wordlistSorted = wordlist.sort_values(by=[1, 0],\n",
    "                                              ascending=[False, True])\n",
    "        print(wordlistSorted)\n",
    "        \n",
    "        freq_word_PerDoc.append(wordlistSorted)\n",
    "        print('\\nfreq_word_PerDoc[', i, '] = ', freq_word_PerDoc[i], '\\n')\n",
    "\n",
    "        i += 1\n",
    "    \n",
    "    print()\n",
    "    print('-----END getPerDocCorpClean METHOD-----')\n",
    "    print()\n",
    "\n",
    "    return(perDocCorpClean, perDocLen, fdistPerDoc, fdistPerDocLen,\n",
    "           freq_word_PerDoc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def getCorpusClean(corpus)\n",
    "# return(corpusClean, corpusLen, fdistCorpus, fdistCorpusLen,\n",
    "#        freq_word_Corpus)\n",
    "\n",
    "# for corpus: tokenize, clean, stem, lem, stopwords, \\\n",
    "# shortwords, etc.\n",
    "\n",
    "\n",
    "def getCorpusClean(corpus):\n",
    "\n",
    "    print()\n",
    "    print('-----BEGIN getCorpusClean METHOD-----')\n",
    "    print()\n",
    "\n",
    "#    i = 0\n",
    "#    for doc in perDocCorp:\n",
    "\n",
    "    # lenDocTokens = 0\n",
    "    # fdist = {}\n",
    "    # lenDocFdist = 0\n",
    "\n",
    "    tokens = str(corpus)\n",
    "    print('tokens = str(corpus)')\n",
    "    print(len(tokens))\n",
    "    print('\\ntokens = ', tokens, '\\n')\n",
    "\n",
    "    # lowecases for content analytics ... we assume, for example, \\\n",
    "    # LOVE is sames love\n",
    "    tokens = tokens.lower()\n",
    "    print('tokens = tokens.lower()')\n",
    "    print(len(tokens))\n",
    "    print('\\ntokens = ', tokens, '\\n')\n",
    "\n",
    "    # the dataset contains useless characters and numbers\n",
    "    # Remove useless numbers and alphanumerical words\n",
    "    # use regular expression ... a-zA-Z0-9 refers to all English \\\n",
    "    # characters (lowercase & uppercase) and numbers\n",
    "    # ^a-zA-Z0-9 is opposite of a-zA-Z0-9\n",
    "    tokens = re.sub(\"[^a-zA-Z0-9]\", \" \", tokens)\n",
    "    print('tokens = re.sub(\"[^a-zA-Z0-9]\", \" \", tokens)')\n",
    "    print(len(tokens))\n",
    "    print('\\ntokens = ', tokens, '\\n')\n",
    "\n",
    "    # tokenization or word split\n",
    "    tokens = word_tokenize(tokens)\n",
    "    print('tokens = word_tokenize(tokens)')\n",
    "    print(len(tokens))\n",
    "    print('\\ntokens = ', tokens, '\\n')\n",
    "\n",
    "    # Filter non-alphanumeric characters from tokens\n",
    "    tokens = [word for word in tokens if word.isalpha()]\n",
    "\n",
    "    # remove short words\n",
    "    tokens = [word for word in tokens if len(word) > 2]\n",
    "\n",
    "    # remove common words\n",
    "    stoplist = stopwords.words('english')\n",
    "    # if you want to remove additional words EXAMPLE\n",
    "    # more = set(['much', 'even', 'time', 'story'])\n",
    "    # more = set(['the'])\n",
    "    # stoplist = set(stoplist) | more\n",
    "    stoplist = set(stoplist) | stopwords_from_file\n",
    "    stoplist = set(stoplist)\n",
    "    tokens = [word for word in tokens if word not in stoplist]\n",
    "\n",
    "    # stemming\n",
    "    stemmer = PorterStemmer()\n",
    "    tokens = [stemmer.stem(word) for word in tokens]\n",
    "    # -----CLEANING COMPLETE-----\n",
    "\n",
    "    corpusClean.append(tokens)\n",
    "    print('\\ncorpusClean = ', tokens, '\\n')\n",
    "\n",
    "    lenCorpusTokens = len(tokens)\n",
    "    print('lenCorpusTokens = ', lenCorpusTokens)\n",
    "    corpusLen.append(lenCorpusTokens)\n",
    "    print(corpusLen)\n",
    "    print('\\ncorpusLen = ', corpusLen, '\\n')\n",
    "\n",
    "    fdist = nltk.FreqDist(tokens)\n",
    "    fdist\n",
    "    fdistCorpus.append(fdist)\n",
    "    print(fdistCorpus)\n",
    "    print('\\nfdistCorpus = ', fdistCorpus, '\\n')\n",
    "    print('\\nfdistCorpus[0].most_common(10) = \\n',\n",
    "          fdistCorpus[0].most_common(10), '\\n')\n",
    "\n",
    "    lenCorpusFdist = len(fdist)\n",
    "    print('lenCorpusFdist = ', lenCorpusFdist)\n",
    "\n",
    "    fdistCorpusLen.append(lenCorpusFdist)\n",
    "    print(fdistCorpusLen)\n",
    "    print('\\nfdistCorpusLen = ', fdistCorpusLen, '\\n')\n",
    "\n",
    "    # freq_word_PerDoc = []\n",
    "    # prepare the results of word frequency on corpus data as a list\n",
    "\n",
    "    freq_word = []\n",
    "\n",
    "    # two values or columns in fdist_a\n",
    "    print()\n",
    "    j = 0\n",
    "    for k, v in fdist.items():\n",
    "        freq_word.append([k, v])\n",
    "        print('freq_word[', j, '] = ', freq_word[j])\n",
    "        j += 1\n",
    "\n",
    "    # make it like an Excel worksheet\n",
    "    wordlist = pd.DataFrame(freq_word)\n",
    "\n",
    "    # pd.set_option('display.max_rows', 1000)\n",
    "    pd.set_option('display.max_rows', 10)\n",
    "    wordlistSorted = wordlist.sort_values(by=[1, 0],\n",
    "                                          ascending=[False, True])\n",
    "#        print(wordlistSorted)\n",
    "    freq_word_Corpus.append(wordlistSorted)\n",
    "    print('\\nfreq_word_Corpus = \\n', freq_word_Corpus, '\\n')\n",
    "\n",
    "#    i += 1\n",
    "\n",
    "    print()\n",
    "    print('-----END getCorpusClean METHOD-----')\n",
    "    print()\n",
    "\n",
    "    return(corpusClean, corpusLen, fdistCorpus, fdistCorpusLen,\n",
    "           freq_word_Corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Indexing Functions"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# ##########################\n",
    "# Step 2: Indexing Functions\n",
    "# ##########################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Postings Dictionary Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create Postings dictionary function\n",
    "# def getPostings(file_names, freq_word_PerDoc, perDocCorpClean)\n",
    "# return(postings)\n",
    "\n",
    "\n",
    "def getPostings(file_names, freq_word_PerDoc, perDocCorpClean):\n",
    "    for docid in (range(len(file_names))):\n",
    "        for word in freq_word_PerDoc[docid][0]:\n",
    "            postings[word][docid] = perDocCorpClean[docid].count(word)\n",
    "    return(postings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create DF Dictionary Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create DF dictionary function\n",
    "# getDF(file_names, freq_word_Corpus, postings)\n",
    "# return(df)\n",
    "\n",
    "\n",
    "def getDF(file_names, freq_word_Corpus, postings):\n",
    "    for docid in (range(len(file_names))):\n",
    "        for word in freq_word_Corpus[0][0]:\n",
    "            df[word] = len(postings[word])\n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Inverted Index & docVecLen Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate IDF\n",
    "# getIDF(word)\n",
    "# return(idf)\n",
    "\n",
    "\n",
    "def getIDF(word):\n",
    "    print('      -----getIDF-----')\n",
    "    print('      word = ', word)\n",
    "\n",
    "    if word in fdistCorpus[0]:\n",
    "        N = (len(file_names))\n",
    "        print('      N = ', N)\n",
    "        dfi = df[word]\n",
    "        print('      dfi = df[word] = ', dfi)\n",
    "        N_div_dfi = N / dfi\n",
    "        print('      N_div_dfi = N / dfi = ', N_div_dfi)\n",
    "        idf = math.log(N_div_dfi, 2)\n",
    "        print('      idf = math.log(N / df[word], 2) = math.log(N / dfi, 2) = math.log(N_div_dfi, 2)')\n",
    "        print('      idf = ', idf)\n",
    "    else:\n",
    "        idf = 0.0\n",
    "        print('      idf = ', idf)\n",
    "    print('      -----back to getWeight-----')\n",
    "    return(idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate weight\n",
    "# getWeight(word, docid)\n",
    "# return(weight)\n",
    "\n",
    "\n",
    "def getWeight(word, docid):\n",
    "    print('    -----getWeight-----')\n",
    "    print('    word = ', word)\n",
    "    print('    docid = ', docid)\n",
    "    tf = 0\n",
    "    idf = 0\n",
    "    print\n",
    "    if docid in postings[word]:\n",
    "        tf = postings[word][docid]\n",
    "        print('    tf = postings[word][docid]')\n",
    "        print('    tf = ', tf)\n",
    "        \n",
    "        idf = getIDF(word)\n",
    "        print('    idf = ', idf)\n",
    "\n",
    "        weight = tf * idf\n",
    "        print('    weight = tf * idf = ', tf, '*', idf, '=', weight)\n",
    "        print('    weight = ', weight)\n",
    "    else:\n",
    "        tf = 0\n",
    "        print('    tf = ', tf)\n",
    "\n",
    "        idf = getIDF(word)\n",
    "        print('    idf = ', idf)\n",
    "\n",
    "#         weight = 0.0\n",
    "        weight = tf * idf\n",
    "        print('    weight = tf * idf = ', tf, '*', idf, '=', weight)\n",
    "        print('    weight = ', weight)\n",
    "    print('    -----back to sumSquares-----')\n",
    "    return(weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create Inverted Index & docVecLen\n",
    "# getDocVecLen(file_names, freq_word_Corpus)\n",
    "# return(docVecLen)\n",
    "\n",
    "\n",
    "def getDocVecLen(file_names, freq_word_Corpus):\n",
    "    for docid in (range(len(file_names))):\n",
    "        print('\\n<<<<<<<<<<Calculate New docVecLen>>>>>>>>>>\\n')\n",
    "        sumSquares = 0\n",
    "        for word in freq_word_Corpus[0][0]:\n",
    "            print('  -----Calculate Update to sumSquares for next Word-----')\n",
    "            print('  word = ', word)\n",
    "            print('  docid = ', docid)\n",
    "\n",
    "            # calculate weight\n",
    "            # getWeight(word, docid)\n",
    "            # return(weight)\n",
    "\n",
    "            weight = getWeight(word, docid)\n",
    "            print('  weight = ', weight)\n",
    "\n",
    "            weight_sq = weight**2\n",
    "            print('  weight_sq = weight**2 = ', weight_sq)\n",
    "            print('  sumSquares = sumSquares + weight_sq = ', sumSquares, '+', weight_sq, '=')\n",
    "\n",
    "            sumSquares += weight_sq\n",
    "            print('  sumSquares = ', sumSquares)\n",
    "            print()\n",
    "        print('  docVecLen[docid=', docid,'] = math.sqrt(sumSquares) = math.sqrt(', sumSquares, ')')\n",
    "\n",
    "        docVecLen[docid] = math.sqrt(sumSquares)\n",
    "        print('  docVecLen[docid=', docid,'] = ', docVecLen[docid])\n",
    "    return(docVecLen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Retrival Functions"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# ##########################\n",
    "# Step 3: Retrival Functions\n",
    "# ##########################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def getQueries(dir_path_queries):\n",
    "# return(queries_from_file)\n",
    "\n",
    "\n",
    "def getQueries(dir_path_queries):\n",
    "    files_queries = os.listdir(dir_path_queries)\n",
    "    for fq in files_queries:\n",
    "        with open(dir_path_queries+'/'+os.path.basename(fq), 'r') as qfile:\n",
    "            queries_from_file = (qfile.read().splitlines())\n",
    "    return(queries_from_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get lines of Input\n",
    "# def getQLines(q)\n",
    "# return(qReview, qDocnum, qTexts)\n",
    "\n",
    "\n",
    "def getQLines(q):\n",
    "    \n",
    "    print('\\n-----BEGIN getQLines METHOD-----')\n",
    "    \n",
    "    # tokenize the words based on white space, removes the punctuation\n",
    "    strtemp = \"\"\n",
    "    \n",
    "    queryNum = 0\n",
    "    qDocnum.append(queryNum)\n",
    "\n",
    "    i = 0\n",
    "    for line in q:\n",
    "        line = line.strip()\n",
    "        strtemp += line\n",
    "        strtemp += \" \"\n",
    "        \n",
    "        qReview.append(line)\n",
    "        i += 1\n",
    "        \n",
    "    qTexts.append(strtemp)\n",
    "    strtemp = \"\"\n",
    "    \n",
    "    print('\\nDone with query = ', q,)\n",
    "\n",
    "    print('\\n-----END getQLines METHOD-----')\n",
    "\n",
    "    return(qReview, qDocnum, qTexts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Query corpus\n",
    "\n",
    "# def getQCorp(qTexts)\n",
    "# return(qCorp)\n",
    "\n",
    "\n",
    "def getQCorp(qTexts):\n",
    "\n",
    "    print('\\n-----BEGIN getQCorp METHOD-----')\n",
    "    \n",
    "    strtemp = \"\"\n",
    "    \n",
    "    for i in range(len(qTexts)):\n",
    "        strtemp += qTexts[i]\n",
    "        print('\\ni = ', i)\n",
    "        print('strtemp = ', strtemp)\n",
    "        qCorp.append(strtemp)\n",
    "        strtemp = \"\"\n",
    "        \n",
    "    print('\\nqCorp = ', qCorp)\n",
    "    \n",
    "    print('\\n-----END getQCorp METHOD-----')\n",
    "    \n",
    "    return(qCorp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean Query corpus\n",
    "\n",
    "# def getQClean(qCorp):\n",
    "# return(qClean, qLen, fdistQ, fdistQLen, \n",
    "#            freq_word_Q, freq_word_Qorpus)\n",
    "\n",
    "# for ea q: tokenize, clean, stem, lem, stopwords, \\\n",
    "# shortwords, etc.\n",
    "\n",
    "\n",
    "def getQClean(qCorp):\n",
    "\n",
    "    print('\\n-----BEGIN getQClean METHOD-----\\n')\n",
    "    \n",
    "#     i = 0\n",
    "#     for doc in qCorp:\n",
    "        \n",
    "#         # lenDocTokens = 0\n",
    "#         # fdist = {}\n",
    "#         # lenDocFdist = 0\n",
    "\n",
    "#         tokens = str(doc)\n",
    "    tokens = str(qCorp)\n",
    "    print('tokens = str(doc)')\n",
    "    print(len(tokens))\n",
    "#     print('\\ntokens [', i, '] = ', tokens, '\\n')\n",
    "    print('\\ntokens = ', tokens, '\\n')\n",
    "\n",
    "    # lowecases for content analytics ... we assume, for example, \\\n",
    "    # LOVE is sames love\n",
    "    tokens = tokens.lower()\n",
    "    print('tokens = tokens.lower()')\n",
    "    print(len(tokens))\n",
    "#     print('\\ntokens [', i, '] = ', tokens, '\\n')\n",
    "    print('\\ntokens = ', tokens, '\\n')\n",
    "    \n",
    "    # the dataset contains useless characters and numbers\n",
    "    # Remove useless numbers and alphanumerical words\n",
    "    # use regular expression ... a-zA-Z0-9 refers to all English \\\n",
    "    # characters (lowercase & uppercase) and numbers\n",
    "    # ^a-zA-Z0-9 is opposite of a-zA-Z0-9\n",
    "    tokens = re.sub(\"[^a-zA-Z0-9]\", \" \", tokens)\n",
    "    print('tokens = re.sub(\"[^a-zA-Z0-9]\", \" \", tokens)')\n",
    "    print(len(tokens))\n",
    "#     print('\\ntokens [', i, '] = ', tokens, '\\n')\n",
    "    print('\\ntokens = ', tokens, '\\n')\n",
    "\n",
    "    # tokenization or word split\n",
    "    tokens = word_tokenize(tokens)\n",
    "    print('tokens = word_tokenize(tokens)')\n",
    "    print(len(tokens))\n",
    "#     print('\\ntokens [', i, '] = ', tokens, '\\n')\n",
    "    print('\\ntokens = ', tokens, '\\n')\n",
    "\n",
    "    # Filter non-alphanumeric characters from tokens\n",
    "    tokens = [word for word in tokens if word.isalpha()]\n",
    "#    print('tokens = [word for word in tokens if word.isalpha()]')\n",
    "#    print(len(tokens))\n",
    "##        print('\\ntokens [', i, '] = ', tokens, '\\n')\n",
    "#    print('\\ntokens = ', tokens, '\\n')\n",
    "\n",
    "    # remove short words\n",
    "    tokens = [word for word in tokens if len(word) > 2]\n",
    "#    print('tokens = [word for word in tokens if len(word) > 2]')\n",
    "#    print(len(tokens))\n",
    "##        print('\\ntokens [', i, '] = ', tokens, '\\n')\n",
    "#    print('\\ntokens = ', tokens, '\\n')\n",
    "\n",
    "    # remove common words\n",
    "    stoplist = stopwords.words('english')\n",
    "    # if you want to remove additional words EXAMPLE\n",
    "#        more = set(['much', 'even', 'time', 'story'])\n",
    "    # more = set(['the'])\n",
    "    # stoplist = set(stoplist) | more\n",
    "\n",
    "    stoplist = set(stoplist) | stopwords_from_file\n",
    "    stoplist = set(stoplist)\n",
    "\n",
    "    tokens = [word for word in tokens if word not in stoplist]\n",
    "#    print('stoplist = set(stoplist)')\n",
    "#    print('tokens = [word for word in tokens if word not in stoplist]')\n",
    "#    print(len(tokens))\n",
    "#    print('\\ntokens [', 0, '] = ', tokens[0], '\\n')\n",
    "#    print('\\ntokens = ', tokens, '\\n')\n",
    "\n",
    "    # stemming\n",
    "    stemmer = PorterStemmer()\n",
    "    tokens = [stemmer.stem(word) for word in tokens]\n",
    "#    print('stemmer = PorterStemmer()')\n",
    "#    print('tokens = [stemmer.stem(word) for word in tokens]')\n",
    "#    print(len(tokens))\n",
    "##        print('\\ntokens [', i, '] = ', tokens, '\\n')\n",
    "#    print('\\ntokens = ', tokens, '\\n')\n",
    "\n",
    "# -----CLEANING COMPLETE-----\n",
    "\n",
    "    qClean.append(tokens)\n",
    "#     print('\\nqClean[', i, '] = ', tokens, '\\n')\n",
    "    print('\\nqClean = ', tokens, '\\n')\n",
    "\n",
    "    lenQTokens = len(tokens)\n",
    "    print('lenQTokens = ', lenQTokens)\n",
    "\n",
    "    qLen.append(lenQTokens)\n",
    "#    print('qLen[i] = ', qLen[i])\n",
    "    print(qLen)\n",
    "#    print('\\nqLen[', i, '] = ', qLen[i], '\\n')\n",
    "    print('\\nqLen = ', qLen, '\\n')\n",
    "\n",
    "    qfdist = nltk.FreqDist(tokens)\n",
    "    print('qfdist = ', qfdist)\n",
    "    print('type(qfdist) = ', type(qfdist))\n",
    "    print('qfdist.items() = \\n', qfdist.items())\n",
    "    print()\n",
    "\n",
    "    fdistQ.append(qfdist)\n",
    "#     print('\\nfdistQ[i] = ', fdistQ[i])\n",
    "    print('fdistQ = ', fdistQ)\n",
    "#     print('\\nfdistQ[', i, '] = ', fdistQ[i], '\\n')\n",
    "    print('\\nfdistQ = ', fdistQ, '\\n')\n",
    "    print('\\nfdistQ[', 0, '] = ', fdistQ[0], '\\n')\n",
    "\n",
    "    print('\\nfdistQ[0].most_common(10) = \\n',\n",
    "          fdistQ[0].most_common(10), '\\n')\n",
    "\n",
    "    lenQFdist = len(qfdist)\n",
    "    print('lenQFdist = ', lenQFdist)\n",
    "    \n",
    "    fdistQLen.append(lenQFdist)\n",
    "    print('fdistQLen = ', fdistQLen)\n",
    "    print('fdistQLen = [0]', fdistQLen[0])\n",
    "#     print('\\fdistQLen[', i, '] = ', fdistQLen[i], '\\n')\n",
    "    print('\\fdistQLen = ', fdistQLen, '\\n')\n",
    "\n",
    "    # prepare the results of word frequency on corpus data as a list\n",
    "#     freq_word_Q = []\n",
    "\n",
    "    # two values or columns in fdist_a\n",
    "    print()\n",
    "    j = 0\n",
    "    for k, v in qfdist.items():\n",
    "        freq_word_Q.append([k, v])\n",
    "        print('freq_word_Q[', j, '] = ', freq_word_Q[j])\n",
    "        j += 1\n",
    "\n",
    "    print('\\n\\n-----DONE WITH FOR K,V-----\\n\\n')\n",
    "            \n",
    "    # make it like an Excel worksheet\n",
    "    # wordlist = pd.DataFrame(freq_word)\n",
    "    qwordlist = pd.DataFrame(freq_word_Q)\n",
    "    print('\\nqwordlist = ', qwordlist)\n",
    "    print()\n",
    "\n",
    "    # pd.set_option('display.max_rows', 1000)\n",
    "    pd.set_option('display.max_rows', 10)\n",
    "\n",
    "#         wordlistSorted = wordlist.sort_values(by=[1, 0],\n",
    "#                                               ascending=[False, True])\n",
    "    qwordlistSorted = qwordlist.sort_values(by=[1, 0],\n",
    "                                          ascending=[False, True])\n",
    "    print('\\nqwordlistSorted = \\n', qwordlistSorted)\n",
    "\n",
    "    freq_word_Qorpus.append(qwordlistSorted)\n",
    "#     print('\\nfreq_word_Q[', i, '] = ', freq_word_Qorpus[i], '\\n')\n",
    "    print('\\nfreq_word_Qorpus[', 0, '] = \\n', freq_word_Qorpus[0], '\\n')\n",
    "    print('\\nfreq_word_Qorpus = \\n', freq_word_Qorpus, '\\n')\n",
    "\n",
    "#         i += 1\n",
    "    \n",
    "    print('\\n-----END getQClean METHOD-----\\n')\n",
    "    \n",
    "    print('freq_word_Q[] = ', freq_word_Q)\n",
    "    print('freq_word_Q[0] = ', freq_word_Q[0])\n",
    "    print('freq_word_Q[1] = ', freq_word_Q[1])\n",
    "    print('freq_word_Q[2] = ', freq_word_Q[2])\n",
    "    \n",
    "    print('\\nfreq_word_Q[0][0] = ', freq_word_Q[0][0])\n",
    "    print()\n",
    "\n",
    "    return(qClean, qLen, fdistQ, fdistQLen,\n",
    "           freq_word_Q, freq_word_Qorpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def getQTuples(freq_word_Q):\n",
    "# return(q_tuple_words, q_tuple_freq_i)\n",
    "\n",
    "\n",
    "def getQTuples(freq_word_Q):\n",
    "\n",
    "    q_tuple_words = tuple([val for (key, val) in enumerate([val for elem in\n",
    "                           freq_word_Q for val in elem]) if key % 2 == 0])\n",
    "\n",
    "    print('q_tuple_words = ', q_tuple_words)\n",
    "    print('type(q_tuple_words) = ', type(q_tuple_words))\n",
    "\n",
    "    q_tuple_freq_i = tuple([val for (key, val) in enumerate([val for elem in\n",
    "                            freq_word_Q for val in elem]) if key % 2 != 0])\n",
    "\n",
    "    print('q_tuple_freq_i = ', q_tuple_freq_i)\n",
    "    print('type(q_tuple_freq_i) = ', type(q_tuple_freq_i))\n",
    "\n",
    "    return(q_tuple_words, q_tuple_freq_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def intersection(post_word_keys):\n",
    "#     return(docid_set)\n",
    "\n",
    "\n",
    "def intersection(post_word_keys):\n",
    "    sets = []\n",
    "    sets = post_word_keys\n",
    "    print('\\nsets = ', sets)\n",
    "\n",
    "#     docid_set = (reduce(set.intersection, [s for s in sets]))\n",
    "    docid_set = (reduce(set.union, [s for s in sets]))\n",
    "    print('\\ndocid_set = ', docid_set)\n",
    "\n",
    "    return(docid_set) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def intersect(a, b):\n",
    "#     return(c)\n",
    "\n",
    "def intersect(a, b):\n",
    "    if len(a) > len(b):\n",
    "        a, b = b, a\n",
    "\n",
    "    c = set()\n",
    "    for x in a:\n",
    "        if x in b:\n",
    "            c.add(x)\n",
    "    return(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate list of relevant documents\n",
    "\n",
    "# def getRetDoc(postings, q_tuple_words)\n",
    "# return(retDoc)\n",
    "\n",
    "\n",
    "def getRetDoc(postings, q_tuple_words):\n",
    "\n",
    "    print('\\npostings = ', postings)\n",
    "    print('\\nq_tuple_words = ', q_tuple_words)\n",
    "\n",
    "    print('\\ntype(postings) = ', type(postings))\n",
    "    print('type(q_tuple_words) = ', type(q_tuple_words))\n",
    "    print()\n",
    "\n",
    "    print('postings.keys() = ', postings.keys())\n",
    "\n",
    "    post_word_keys = ([set(postings[word].keys()) for word in q_tuple_words])\n",
    "    print('\\npost_word_keys = ', post_word_keys)\n",
    "\n",
    "#     post_word_keys = ([(postings[word].keys()) for word in q_tuple_words])\n",
    "#     print('\\npost_word_keys = ', post_word_keys)\n",
    "\n",
    "    # def intersection(post_word_keys):\n",
    "    #     return(docid_set)\n",
    "\n",
    "    docid_set = intersection(post_word_keys)\n",
    "    print('\\ndocid_set = ', docid_set)\n",
    "    print()\n",
    "\n",
    "    retDoc = docid_set\n",
    "    print('\\nretDoc = ', retDoc)\n",
    "    print()\n",
    "\n",
    "    return(retDoc)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# def getQVecLen(docid, q_tuple_words,\n",
    "#               q_tuple_freq_i, fdistCorpus):\n",
    "# return(cosSim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def getCosSim(docid, q_tuple_words,\n",
    "#               q_tuple_freq_i, fdistCorpus):\n",
    "# return(cosSim)\n",
    "\n",
    "def getCosSim(docid, q_tuple_words,\n",
    "              q_tuple_freq_i, fdistCorpus):\n",
    "    similarity = 0.0\n",
    "    cosSim = 0.0\n",
    "    qTF = 0\n",
    "    qIDF = 0.0\n",
    "    qWeight = 0.0\n",
    "    qWeightSquared = 0.0\n",
    "    qSumWeightSquared = 0.0\n",
    "    global qVecLen\n",
    "    docWordWeight = 0.0\n",
    "    x = 0\n",
    "    for word in q_tuple_words:\n",
    "#         qTF = q_tuple_freq_i[x]\n",
    "        if word in fdistCorpus[0]:\n",
    "            print('word = ', word)\n",
    "            qTF = q_tuple_freq_i[x]\n",
    "            print('qTF = ', qTF)\n",
    "            qIDF = getIDF(word)\n",
    "            print('qIDF = ', qIDF)\n",
    "            qWeight = qTF * qIDF\n",
    "            print('qWeight = ', qWeight)\n",
    "            qWeightSquared = qWeight**2\n",
    "            qSumWeightSquared += qWeightSquared\n",
    "\n",
    "            docWordWeight = getWeight(word, docid)\n",
    "\n",
    "            similarity += qWeight * docWordWeight\n",
    "        x += 1\n",
    "\n",
    "#     print('\\ndocid = ', docid)\n",
    "    \n",
    "    qVecLen = math.sqrt(qSumWeightSquared)\n",
    "#     print('\\nqVecLen = ', qVecLen)\n",
    "\n",
    "    cosSim = similarity / (qVecLen * docVecLen[docid])\n",
    "#     print('\\ncosSim = ', cosSim)\n",
    "\n",
    "    return(cosSim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def getCosSimScoresList(retDoc, q_tuple_words,\n",
    "#                         q_tuple_freq_i, fdistCorpus):\n",
    "# return(cosSimScoresList)\n",
    "\n",
    "def getCosSimScoresList(retDoc, q_tuple_words,\n",
    "                        q_tuple_freq_i, fdistCorpus):\n",
    "\n",
    "    cosSimScoresList = [\n",
    "        (docid+1, getCosSim(docid, q_tuple_words, q_tuple_freq_i, fdistCorpus)) \n",
    "        for docid in retDoc]\n",
    "\n",
    "    return(cosSimScoresList)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Ranking Functions"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# ##########################\n",
    "# Step 4: Ranking Functions\n",
    "# ##########################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def getRankCosSimList(cosSimScoresList)\n",
    "# return(rankCosSimList)\n",
    "\n",
    "\n",
    "def getRankCosSimList(cosSimScoresList):\n",
    "    rankCosSimList = sorted(cosSimScoresList, key=lambda l: l[1], reverse=True)\n",
    "    print('\\nrankCosSimList = \\n', rankCosSimList)\n",
    "    return(rankCosSimList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#    def getRankListPerQ(qNum, queries_from_file,\n",
    "#                        postings, fdistCorpus)\n",
    "#    return(rankListPerQ)\n",
    "\n",
    "\n",
    "def getRankListPerQ(qNum, queries_from_file,\n",
    "                    postings, fdistCorpus):\n",
    "\n",
    "    # def getQLines(q)\n",
    "    # return(qReview, qDocnum, qTexts)\n",
    "    global q\n",
    "    q = []\n",
    "    print('q = ', q)\n",
    "    global qReview\n",
    "    qReview = []\n",
    "    print('qReview = ', qReview)\n",
    "    global qDocnum\n",
    "    qDocnum = []\n",
    "    print('qDocnum = ', qDocnum)\n",
    "    # qTitles = []\n",
    "    global qTexts\n",
    "    qTexts = []\n",
    "    print('qTexts = ', qTexts)\n",
    "\n",
    "    # def getQCorp(qTexts)\n",
    "    # return(qCorp)\n",
    "    global qCorp\n",
    "    qCorp = []\n",
    "    print('qCorp = ', qCorp)\n",
    "\n",
    "    # def getQClean(qCorp):\n",
    "    # return(qClean, qLen, fdistQ, fdistQLen,\n",
    "    #            freq_word_Q, freq_word_Qorpus)\n",
    "    global qClean\n",
    "    qClean = []\n",
    "    print('qClean = ', qClean)\n",
    "    global qLen\n",
    "    qLen = []\n",
    "    print('qLen = ', qLen)\n",
    "    global fdistQ\n",
    "    fdistQ = []\n",
    "    print('fdistQ = ', fdistQ)\n",
    "    global fdistQLen\n",
    "    fdistQLen = []\n",
    "    print('fdistQLen = ', fdistQLen)\n",
    "    global freq_word_Q\n",
    "    freq_word_Q = []\n",
    "    print('freq_word_Q = ', freq_word_Q)\n",
    "    global freq_word_Qorpus\n",
    "    freq_word_Qorpus = []\n",
    "    print('freq_word_Qorpus = ', freq_word_Qorpus)\n",
    "\n",
    "    # def getRetDoc(postings, q_tuple_words)\n",
    "    # return(retDoc)\n",
    "    global retDoc\n",
    "    retDoc = []\n",
    "    print('retDoc = ', retDoc)\n",
    "\n",
    "    # def getCosSimScoresList(retDoc, q_tuple_words,\n",
    "    #                         q_tuple_freq_i, fdistCorpus):\n",
    "    # return(cosSimScoresList)\n",
    "    cosSimScoresList = defaultdict(float)\n",
    "\n",
    "    # def getRankCosSimList(cosSimScoresList)\n",
    "    # return(rankCosSimList)\n",
    "    global rankCosSimList\n",
    "    rankCosSimList = []\n",
    "    print('rankCosSimList = ', rankCosSimList)\n",
    "\n",
    "    query = []\n",
    "    print('query = ', query)\n",
    "    input = \"\"\n",
    "\n",
    "    input = queries_from_file[qNum]\n",
    "    print('input = queries_from_file[', qNum, '] = ', queries_from_file[qNum])\n",
    "    print('input = ', input)\n",
    "\n",
    "    query.append(input)\n",
    "    q = query\n",
    "\n",
    "    # def getQLines(q)\n",
    "    # return(qReview, qDocnum, qTexts)\n",
    "\n",
    "    qReview, qDocnum, qTexts = getQLines(q)\n",
    "\n",
    "    print('\\nqTexts = ', qTexts)\n",
    "    print()\n",
    "    print('DONE ASSIGNING DOCNUM TITLES AND TEXTS')\n",
    "    print('\\n-----END OF getQLines-----')\n",
    "\n",
    "    # def getQCorp(qTexts)\n",
    "    # return(qCorp)\n",
    "\n",
    "    qCorp = getQCorp(qTexts)\n",
    "\n",
    "    # for ea qCorp: tokenize, clean, stem, lem, stopwords,\\\n",
    "    # shortwords, etc.\n",
    "\n",
    "    # def getQClean(qCorp):\n",
    "    # return(qClean, qLen, fdistQ, fdistQLen,\n",
    "    #            freq_word_Q, freq_word_Qorpus)\n",
    "\n",
    "    qClean, qLen, fdistQ, fdistQLen, freq_word_Q, freq_word_Qorpus\\\n",
    "        = getQClean(qCorp)\n",
    "\n",
    "    x = 0\n",
    "    for x in range(len(qClean)):\n",
    "        print()\n",
    "#        print('qClean[', x, '] = ', qClean[x])\n",
    "#        print('qLen[', x, '] = ', qLen[x])\n",
    "        print('++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++')\n",
    "        print('++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++')\n",
    "        print('\\nfdistQ[', x, '] = ', fdistQ[x])\n",
    "        print('\\nfdistQLen[', x, '] = ', fdistQLen[x])\n",
    "        print('\\nfreq_word_Q[', x, '] = \\n', freq_word_Q[x])\n",
    "        print('\\nfreq_word_Qorpus[', x, '] = \\n', freq_word_Qorpus[x])\n",
    "        print()\n",
    "        print('++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++')\n",
    "        print('++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++')\n",
    "        print()\n",
    "        x += 1\n",
    "\n",
    "    print('type(fdistQ) = ', type(fdistQ))\n",
    "    print('type(freq_word_Q) = ', type(freq_word_Q))\n",
    "    print('type(freq_word_Qorpus) = ', type(freq_word_Qorpus))\n",
    "    print('\\nfdistQ = ', fdistQ)\n",
    "    print('\\nfreq_word_Q = ', freq_word_Q)\n",
    "    print('\\nfreq_word_Qorpus = \\n', freq_word_Qorpus)\n",
    "    print()\n",
    "\n",
    "    print('\\nfreq_word_Q[:] = \\n', freq_word_Q[:], '\\n')\n",
    "    print('\\ntype(freq_word_Q) = ', type(freq_word_Q))\n",
    "\n",
    "    qClean0 = str(qClean[0])\n",
    "\n",
    "    print('\\nqClean0 =\\n', qClean0)\n",
    "    print('type(qClean0) = ', type(qClean0))\n",
    "\n",
    "    # def getQTuples(freq_word_Q):\n",
    "    # return(q_tuple_words, q_tuple_freq_i)\n",
    "\n",
    "    q_tuple_words, q_tuple_freq_i = getQTuples(freq_word_Q)\n",
    "\n",
    "    # def getRetDoc(postings, q_tuple)\n",
    "    # return(retDoc)\n",
    "\n",
    "#    print('\\npostings = ', postings)\n",
    "\n",
    "    retDoc = getRetDoc(postings, q_tuple_words)\n",
    "\n",
    "    print('\\nq_tuple_words = ', q_tuple_words)\n",
    "    print('\\nq_tuple_freq_i = ', q_tuple_freq_i)\n",
    "    print('\\nfdistCorpus = ', fdistCorpus)\n",
    "\n",
    "    cosSimScoresList = getCosSimScoresList(retDoc, q_tuple_words,\n",
    "                                           q_tuple_freq_i, fdistCorpus)\n",
    "\n",
    "    '''\n",
    "    cosSimScoresList[ 0 ] =  (1, 0.016380446956997974)\n",
    "    cosSimScoresList[ 1 ] =  (2, 0.18149296974189377)\n",
    "    cosSimScoresList[ 2 ] =  (4, 0.130410097650556)\n",
    "    cosSimScoresList[ 3 ] =  (5, 0.1191294611973798)\n",
    "    cosSimScoresList[ 4 ] =  (6, 0.05303561515807138)\n",
    "    '''\n",
    "\n",
    "    print('\\nqVecLen = ', qVecLen)\n",
    "    print()\n",
    "\n",
    "    print('\\nOUTPUT of cosSimScoresList values')\n",
    "    print()\n",
    "    for z in xrange(5):\n",
    "        print('cosSimScoresList[', z, '] = ', cosSimScoresList[z])\n",
    "\n",
    "    # def getRankCosSimList(cosSimScoresList)\n",
    "    # return(rankCosSimList)\n",
    "\n",
    "    rankCosSimList = getRankCosSimList(cosSimScoresList)\n",
    "\n",
    "    print('\\ntype(rankCosSimList) = ', type(rankCosSimList))\n",
    "\n",
    "    '''\n",
    "    rankCosSimList[ 0 ] =  (323, 0.36102506311675114)\n",
    "    rankCosSimList[ 1 ] =  (322, 0.3515479432650447)\n",
    "    rankCosSimList[ 2 ] =  (1394, 0.3512276970366803)\n",
    "    rankCosSimList[ 3 ] =  (628, 0.3464697753459441)\n",
    "    rankCosSimList[ 4 ] =  (179, 0.310596542107325)\n",
    "\n",
    "    VERSUS [(docid + 1) in cosSimScoresList]\n",
    "\n",
    "    rankCosSimList[ 0 ] =  (324, 0.36102506311675114)\n",
    "    rankCosSimList[ 1 ] =  (323, 0.3515479432650447)\n",
    "    rankCosSimList[ 2 ] =  (1395, 0.3512276970366803)\n",
    "    rankCosSimList[ 3 ] =  (629, 0.3464697753459441)\n",
    "    rankCosSimList[ 4 ] =  (180, 0.310596542107325)\n",
    "    '''\n",
    "\n",
    "    print('len(rankCosSimList) = ', len(rankCosSimList))\n",
    "#    print()\n",
    "\n",
    "    print()\n",
    "    for y in xrange(5):\n",
    "        print('rankCosSimList[', y, '] = ', rankCosSimList[y])\n",
    "\n",
    "    rankListPerQ = rankCosSimList\n",
    "\n",
    "    print()\n",
    "    for z in xrange(5):\n",
    "        print('rankListPerQ[', z, '] = ', rankListPerQ[z])\n",
    "\n",
    "    return(rankListPerQ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sendToOutputFolder(dir_path_output, output_qid_docid)\n",
    "\n",
    "def sendToOutputFolder(dir_path_output, output_qid_docid):\n",
    "    files_ouput = os.listdir(dir_path_output)\n",
    "    for fo in files_ouput:\n",
    "        with open(dir_path_output+'/'+os.path.basename(fo), 'w') as ofile:\n",
    "            ofile.write('\\n'.join('{} {}'.format(qiddocid[0], qiddocid[1]) for\n",
    "                                  qiddocid in output_qid_docid))\n",
    "    return()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def getRelevance(dir_path_relevance):\n",
    "# return(relevance_from_file)\n",
    "\n",
    "# relevance_from_file = getRelevance(dir_path_relevance)\n",
    "\n",
    "\n",
    "def getRelevance(dir_path_relevance):\n",
    "    files_relevance = os.listdir(dir_path_relevance)\n",
    "    for fr in files_relevance:\n",
    "        with open(dir_path_relevance+'/'+os.path.basename(fr), 'r') as rfile:\n",
    "#            relevance_from_file = (rfile.read().splitlines())\n",
    "            relevance_from_file = [tuple(int(n) for n in line.split())\n",
    "                                   for line in rfile]\n",
    "    return(relevance_from_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def getQtyRelDocPerQ(relevance_from_file):\n",
    "# return(qtyRelDocPerQ)\n",
    "\n",
    "# qtyRelDocPerQ = getQtyRelDocPerQ(relevance_from_file)\n",
    "\n",
    "\n",
    "def getQtyRelDocPerQ(relevance_from_file):\n",
    "    qNum = 1\n",
    "    qtyRelDoc = 0\n",
    "    totRelDoc = len(relevance_from_file)\n",
    "    print('totRelDoc = ', totRelDoc)\n",
    "    for n in xrange(totRelDoc):\n",
    "        print('\\nn = ', n)\n",
    "        if (n == totRelDoc - 1) & (relevance_from_file[n][0] == qNum):\n",
    "            qtyRelDoc += 1\n",
    "            print('qtyRelDoc = ', qtyRelDoc)\n",
    "            print('A Q DOC = ', qNum, ' ', relevance_from_file[n][1])\n",
    "            qtyRelDocPerQ.append(qtyRelDoc)\n",
    "            print('\\nqtyRelDocPerQ) = ', qtyRelDocPerQ)\n",
    "        elif relevance_from_file[n][0] == qNum:\n",
    "            qtyRelDoc += 1\n",
    "            print('qtyRelDoc = ', qtyRelDoc)\n",
    "            print('B Q DOC = ', qNum, ' ', relevance_from_file[n][1])\n",
    "        elif relevance_from_file[n][0] == qNum + 1:\n",
    "            qtyRelDocPerQ.append(qtyRelDoc)\n",
    "            print('\\nqtyRelDocPerQ = ', qtyRelDocPerQ, '\\n')\n",
    "            qtyRelDoc = 1\n",
    "            print('qtyRelDoc = ', qtyRelDoc)\n",
    "            qNum += 1\n",
    "            print('C Q DOC = ', qNum, ' ', relevance_from_file[n][1])\n",
    "    return(qtyRelDocPerQ)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# def getWordCount(tokens):\n",
    "#    for doc in review:\n",
    "#        tf = Counter()\n",
    "#        for word in doc.split():\n",
    "#            tf[word] += 1\n",
    "#        print()\n",
    "#        print(tf.items())\n",
    "#        cnt_dict = tf\n",
    "#        print('cnt_dict = ', cnt_dict)\n",
    "#    print(tf)\n",
    "#    print()\n",
    "#    return(tf, cnt_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# #############################################\n",
    "# #### BEGIN MAIN PART OF PROGRAM #####\n",
    "# #############################################"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# #################################\n",
    "# MAIN\n",
    "# #################################"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "if __name__ == '__main__':"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# *************************\n",
    "# ---BEGIN Declare Variables---\n",
    "# *************************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print()\n",
    "#-----------------\n",
    "# constants\n",
    "############################################################\n",
    "# ______path to cranfieldDocs directory_____\n",
    "\n",
    "# dir_path = 'C:/Users/Derek Christensen/Dropbox/_cis833irtm/hw2//\\\n",
    "    # cranfieldDocs'\n",
    "# dir_path = 'C:/Users/Derek Christensen/Dropbox/_cis833irtm/hw2/data'\n",
    "# dir_path = 'C:/Users/Derek Christensen/Dropbox/_cis833irtm/hw2/data-temp'\n",
    "# dir_path = 'C:/Users/Derek Christensen/Dropbox/_cis833irtm/hw2/data-misc'\n",
    "\n",
    "dir_path = 'C:/Users/derekc/Dropbox/__cis833irtm/hw2/cranfieldDocs'\n",
    "\n",
    "# dir_path = r'C:/Users/derekc/Dropbox/__cis833irtm/hw2/data-1'\n",
    "# dir_path = r'C:/Users/derekc/Dropbox/__cis833irtm/hw2/data-fox'\n",
    "# dir_path = r'C:/Users/derekc/Dropbox/__cis833irtm/hw2/data-fox2'\n",
    "# dir_path = r'C:/Users/derekc/Dropbox/__cis833irtm/hw2/data'\n",
    "# dir_path = r'C:/Users/derekc/Dropbox/__cis833irtm/hw2/data-15'\n",
    "# dir_path = r'C:/Users/derekc/Dropbox/__cis833irtm/hw2/data-Q2'\n",
    "# dir_path = r'C:/Users/derekc/Dropbox/__cis833irtm/hw2/data-Q2-2'\n",
    "# dir_path = r'C:/Users/derekc/Dropbox/__cis833irtm/hw2/data-Q2-3'\n",
    "\n",
    "dir_path_stopwords = r'C:/Users/derekc/Dropbox/__cis833irtm/hw2/stopwords'\n",
    "\n",
    "dir_path_queries = r'C:/Users/derekc/Dropbox/__cis833irtm/hw2/queries'\n",
    "# dir_path_queries = r'C:/Users/derekc/Dropbox/__cis833irtm/hw2/queries2'\n",
    "# dir_path_queries = r'C:/Users/derekc/Dropbox/__cis833irtm/hw2/Q2'\n",
    "\n",
    "dir_path_output = r'C:/Users/derekc/Dropbox/__cis833irtm/hw2/output'\n",
    "\n",
    "dir_path_relevance = r'C:/Users/derekc/Dropbox/__cis833irtm/hw2/relevance'\n",
    "# dir_path_relevance = r'C:/Users/derekc/Dropbox/__cis833irtm/hw2/rel-Q2'\n",
    "# dir_path_relevance = r'C:/Users/derekc/Dropbox/__cis833irtm/hw2/rel-Q2-2'\n",
    "# dir_path_relevance = r'C:/Users/derekc/Dropbox/__cis833irtm/hw2/rel-Q123'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\nInput for dir_path_queries folder: = ', dir_path_queries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# declare arrays, variables\n",
    "\n",
    "# def getFiles(dir_path)\n",
    "# return(files, file_names, file_idx, file_zip, file_dict, file_dict_enum)\n",
    "\n",
    "global stopwords_from_file\n",
    "global queries_from_file\n",
    "\n",
    "files = []\n",
    "file_names = []\n",
    "\n",
    "file_idx = []\n",
    "file_zip = []\n",
    "file_dict = []\n",
    "file_dict = {}\n",
    "file_dict_enum = {}\n",
    "\n",
    "# def getLines(files, dir_path)\n",
    "# return(review, docnum, titles, texts) \n",
    "review = []\n",
    "docnum = []\n",
    "titles = []\n",
    "texts = []\n",
    "tf = []\n",
    "j = 0\n",
    "\n",
    "# def getPerDocCorp(titles, texts)\n",
    "# return(perDocCorp, corpus)\n",
    "perDocCorp = []\n",
    "corpus = []\n",
    "\n",
    "# def getPerDocCorpClean(perDocCorp)\n",
    "# return(perDocCorpClean, perDocLen, fdistPerDoc, fdistPerDocLen,\n",
    "#       freq_word_PerDoc)\n",
    "perDocCorpClean = []\n",
    "perDocLen = []\n",
    "fdistPerDoc = []\n",
    "fdistPerDocLen = []\n",
    "freq_word_PerDoc = []\n",
    "\n",
    "# def getCorpusClean(corpus)\n",
    "# return(corpusClean, corpusLen, fdistCorpus, fdistCorpusLen,\n",
    "#        freq_word_Corpus)\n",
    "corpusClean = []\n",
    "corpusLen = []\n",
    "fdistCorpus = []\n",
    "fdistCorpusLen = []\n",
    "freq_word_Corpus = []\n",
    "\n",
    "# def getPostings(file_names, freq_word_PerDoc, perDocCorpClean)\n",
    "# return(postings)\n",
    "postings = defaultdict(dict)\n",
    "\n",
    "# getDF(file_names, freq_word_Corpus, postings)\n",
    "# return(df)\n",
    "df = defaultdict(int)\n",
    "\n",
    "# getDocVecLen(file_names, freq_word_Corpus)\n",
    "# return(docVecLen)\n",
    "\n",
    "docVecLen = defaultdict(float)\n",
    "\n",
    "# def getRankListPerQ(qNum, queries_from_file,\n",
    "#                     postings, fdistCorpus)\n",
    "# return(rankListPerQ)\n",
    "rankListPerQ = []\n",
    "global output_qid_docid\n",
    "output_qid_docid = []\n",
    "\n",
    "# def getQtyRelDocPerQ(relevance_from_file):\n",
    "# return(qtyRelDocPerQ)\n",
    "\n",
    "global qtyRelDocPerQ\n",
    "qtyRelDocPerQ = []"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# *************************\n",
    "# ---END Declare Variables---\n",
    "# *************************"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1. Preprocessing  \n",
    "Write a program that preprocesses the collection.<br>\n",
    "This preprocessing stage should specifically include a function that tokenizes the text.<br>\n",
    "In doing so, tokenize on whitespace and remove punctuation.<br>\n",
    "#### Note that you also need to eliminate the SGML tags (e.g., '<'TITLE>', '<'DOC>, '<'TEXT>, etc.) - you should only keep the actual title and text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## • Input: Documents that are read one by one from the collection\n",
    "### • Implement the preprocessing functions:  \n",
    ">#### • For tokenization  \n",
    ">#### • For stop word removal  \n",
    ">#### • For stemming  \n",
    "## • Output: Tokens to be added to the index  \n",
    ">#### • No punctuation, no stop-words, stemmed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## • Input: Documents that are read one by one from the collection"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# #######################################\n",
    "# ###### STEP 1: PREPROCESSING   ########\n",
    "# #######################################"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# #############################################\n",
    "# ####### OBTAINING DATA FILES ########\n",
    "# #############################################"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# *************************\n",
    "# ---BEGIN getStopwords---\n",
    "# *************************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# def getStopwords(dir_path_stopwords)\n",
    "# return(stopwords_from_file)\n",
    "\n",
    "stopwords_from_file = getStopwords(dir_path_stopwords)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# *************************\n",
    "# ---END getStopwords---\n",
    "# *************************"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# *************************\n",
    "# ---BEGIN getFiles---\n",
    "# *************************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# get all files inside the directory & process to arrays & dicts\n",
    "# getFiles(dir_path, files, file_names, file_idx, file_zip, file_dict)\n",
    "# print(files)\n",
    "#\n",
    "# def getFiles(dir_path)\n",
    "# return(files, file_names, file_idx, file_zip, file_dict, files_dict_enum)\n",
    "\n",
    "files, file_names, file_idx, file_zip, file_dict, file_dict_enum = \\\n",
    "    getFiles(dir_path)\n",
    "\n",
    "print('files = ', files)\n",
    "print('file_names = ', file_names)\n",
    "print('len(file_names) = ', len(file_names))\n",
    "print()\n",
    "print('file_idx = ', file_idx)\n",
    "print()\n",
    "print('file_zip = ', file_zip)\n",
    "print()\n",
    "print('file_dict = ', file_dict)\n",
    "print()\n",
    "print('files_dict_enum = ', file_dict_enum)\n",
    "print()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# *************************\n",
    "# ---END getFiles---\n",
    "# *************************"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## • Implement the preprocessing functions:  \n",
    ">### • For tokenization  \n",
    ">### • For stop word removal  \n",
    ">### • For stemming  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eliminate SGML tags & only keep TITLE & TEXT"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# #####################################################################\n",
    "# ########### Eliminate SGML tags & only keep TITLE & TEXT ############\n",
    "# #####################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading data as list & eliminate SGML tags"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# ###########################################################\n",
    "# ####### READING DATA AS LIST & ELIMINATE SGML TAGS ########\n",
    "# ###########################################################"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# *************************\n",
    "# ---BEGIN getLines---\n",
    "# *************************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# start processing the ipfile & break all files into lines\n",
    "#\n",
    "# def getLines(files, dir_path):\n",
    "# return(review, docnum, titles, texts)\n",
    "\n",
    "review, docnum, titles, texts = getLines(files, dir_path)\n",
    "\n",
    "print('REVIEW = \\n', review)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('review = ', review)\n",
    "print()\n",
    "print('docnum = ', docnum)\n",
    "print()\n",
    "print('titles = ', titles)\n",
    "print()\n",
    "print('texts = ', texts)\n",
    "print()\n",
    "print('DONE ASSIGNING DOCNUM TITLES AND TEXTS')\n",
    "print()\n",
    "print('\\n-----END OF getLines-----\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print 1st 2 tokens in list review\n",
    "print('review[:2] = ', review[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('review[0] = ', review[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('review[0:3] = ', review[0:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of tokens = len(review) = ', len(review))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# *************************\n",
    "# ---END getLines---\n",
    "# *************************"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge Titles and Texts"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# #####################################\n",
    "# ###### GET EACH DOCs CORPUS #########\n",
    "# #####################################"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# *************************\n",
    "# ---BEGIN getPerDocCorp---\n",
    "# *************************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# merge TITLES & TEXTS into 1 STRING per DOC\n",
    "\n",
    "# def getPerDocCorp(titles, texts)\n",
    "# return(perDocCorp, corpus)\n",
    "\n",
    "perDocCorp, corpus = getPerDocCorp(titles, texts)\n",
    "\n",
    "print()\n",
    "print('perDocCorp = \\n', perDocCorp)\n",
    "print()\n",
    "print('corpus = \\n', corpus)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# //////////////////////////////////////////////////\n",
    "#    PRINT EACH DOC'S CORP\n",
    "# \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n",
    "\n",
    "for i in range(len(perDocCorp)):\n",
    "    print('\\nperDocCorp[', i, '] = ', perDocCorp[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print()\n",
    "print('perDocCorp[0:1] = \\n', perDocCorp[0:1])\n",
    "print()\n",
    "print('perDocCorp[1:2] = \\n', perDocCorp[1:2])\n",
    "print()\n",
    "print('perDocCorp[2:3] = \\n', perDocCorp[2:3])\n",
    "print()\n",
    "print('perDocCorp[3:4] = \\n', perDocCorp[3:4])\n",
    "print()\n",
    "print('perDocCorp[4:5] = \\n', perDocCorp[4:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print()\n",
    "print('perDocCorp[:1] = \\n', perDocCorp[:1])\n",
    "print()\n",
    "print('perDocCorp[0:2] = \\n', perDocCorp[0:2])\n",
    "print()\n",
    "print('perDocCorp[0:3] = \\n', perDocCorp[0:3])\n",
    "print()\n",
    "print()\n",
    "print('perDocCorp[2:4] = \\n', perDocCorp[2:4])\n",
    "print()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# *************************\n",
    "# --END getPerDocCorp--\n",
    "# *************************"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean Corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## • Output: Tokens to be added to the index  \n",
    ">### • No punctuation, no stop-words, stemmed"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# #######################################\n",
    "# ###### CLEAN EACH DOCs CORPUS #########\n",
    "# #######################################"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# *************************\n",
    "# --BEGIN perDocCorpCLean--\n",
    "# *************************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# for ea perDocCorp: tokenize, clean, stem, lem, stopwords, \\\n",
    "# shortwords, etc.\n",
    "\n",
    "# def getPerDocCorpClean(perDocCorp)\n",
    "# return(perDocCorpClean, perDocLen, fdistPerDoc, fdistPerDocLen,\n",
    "#   freq_word_PerDoc)\n",
    "\n",
    "perDocCorpClean, perDocLen, fdistPerDoc, fdistPerDocLen, freq_word_PerDoc \\\n",
    "    = getPerDocCorpClean(perDocCorp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(perDocCorpClean[0:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(perDocCorpClean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('\\nperDocCorpClean =\\n', perDocCorpClean, '\\n')\n",
    "x = 0\n",
    "for x in range(len(perDocCorpClean)):\n",
    "    print('perDocCorpClean[', x, '] = ', perDocCorpClean[x])\n",
    "    print('perDocLen[', x, '] = ', perDocLen[x])\n",
    "    print('fdistPerDoc[', x, '] = ', fdistPerDoc[x])\n",
    "    print('fdistPerDocLen[', x, '] = ', fdistPerDocLen[x])\n",
    "    print('freq_word_PerDoc[', x, '] = \\n', freq_word_PerDoc[x])\n",
    "    print()\n",
    "    x += 1\n",
    "\n",
    "print('len(perDocCorpClean) = ', len(perDocCorpClean))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fdistPerDoc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# //////////////////////////////////////////////////\n",
    "#    fdistPerDoc[2]\n",
    "# \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n",
    "\n",
    "print('fdistPerDoc[ 2 ] = ', fdistPerDoc[ 2 ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fdistPerDoc[ 2 ]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "fdistPerDoc[2][:]\n",
    "\n",
    "# TypeErrorTraceback (most recent call last)\n",
    "# <ipython-input-104-d07dc7f30930> in <module>()\n",
    "# ----> 1 fdistPerDoc[2][:]\n",
    "\n",
    "# TypeError: unhashable type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# //////////////////////////////////////////////////////////////////////\n",
    "# doc156words = {k: fdistPerDoc[2][k] for k in fdistPerDoc[2].keys()[:]}\n",
    "# \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n",
    "\n",
    "doc156words = {k: fdistPerDoc[2][k] for k in fdistPerDoc[2].keys()[:]}\n",
    "print('len(doc156words) = ', len(doc156words))\n",
    "doc156words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\ndoc156words =\\n', doc156words)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# *************************\n",
    "# --END perDocCorpClean--\n",
    "# *************************"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# #######################################\n",
    "# ######## CLEAN ENTIRE CORPUS ##########\n",
    "# #######################################"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "corpus = \n",
    " ['\"The quick brown fox jumps over the lazy dog\" \"The fox jumps over the dog and the Python\", \"The fox is very clever\" \"The fox is very clever and quick and the dog is slow and lazy\",. \"The cat is smarter \" \"The cat is smarter than the fox and the dog and her name is Ruby\", \"excellent programming language!?!?\" \"Python is an excellent programming language and Python programs are smaller than Java programs\" Web Site Programming \"Java and Ruby are other programming languages for Web Site development and are also not for dogs\", popular programming languages for Data Science internal . \"Python and Java are very popular programming languages for Data Science\", ']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "corpus"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# *************************\n",
    "# --BEGIN corpusClean--\n",
    "# *************************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# for corpus: tokenize, clean, stem, lem, stopwords, \\\n",
    "# shortwords, etc.\n",
    "\n",
    "# def getCorpusClean(corpus)\n",
    "# return(corpusClean, corpusLen, fdistCorpus, fdistCorpusLen,\n",
    "#        freq_word_Corpus)\n",
    "\n",
    "corpusClean, corpusLen, fdistCorpus, fdistCorpusLen, freq_word_Corpus\\\n",
    "    = getCorpusClean(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('\\ncorpusClean =\\n', corpusClean, '\\n')\n",
    "x = 0\n",
    "for x in range(len(corpusClean)):\n",
    "    print('corpusClean[', x, '] = ', corpusClean[x])\n",
    "    print('corpusLen[', x, '] = ', corpusLen[x])\n",
    "    print('fdistCorpus[', x, '] = ', fdistCorpus[x])\n",
    "    print('fdistCorpusLen[', x, '] = ', fdistCorpusLen[x])\n",
    "    print('freq_word_Corpus[', x, '] = \\n', freq_word_Corpus[x])\n",
    "    print()\n",
    "    x += 1\n",
    "\n",
    "print('len(corpusClean) = ', len(corpusClean))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\nfdistCorpus = ', fdistCorpus)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fdistCorpus"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# *************************\n",
    "# --END corpusClean--\n",
    "# *************************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('files = ', files)\n",
    "print('file_names = ', file_names)\n",
    "print('len(file_names) = ', len(file_names))\n",
    "print()\n",
    "print('file_idx = ', file_idx)\n",
    "print()\n",
    "print('file_zip = ', file_zip)\n",
    "print()\n",
    "# print('file_dict = ', file_dict)\n",
    "# print()\n",
    "# print('files_dict_enum = ', file_dict_enum)\n",
    "# print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Indexing  \n",
    "1. Implement an indexing scheme based on the vector space model, as discussed in class. The\n",
    "steps pointed out in class can be used as guidelines for the implementation. For the weighting\n",
    "scheme, use and experiment with:  \n",
    "• TF-IDF (do not divide TF by the maximum term frequency in a document)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### • Build an inverted index, with an entry for each word in the vocabulary\n",
    "## • Input: Tokens obtained from the preprocessing module\n",
    "## • Output: An inverted index for fast access\n",
    "### • Many data structures are appropriate for fast access\n",
    ">#### • We will use hashtables\n",
    "        * Store tokens in hashtable, with token string as key and weight as value.\n",
    "        * Table must fit in main memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### • We need:\n",
    ">#### • One entry for each word in the vocabulary\n",
    ">#### • For each such entry:\n",
    "    * Keep a list of all the documents where it appears together with the corresponding frequency --> TF\n",
    "    * Keep the total number of documents in which the corresponding word appears --> IDF\n",
    "### • Constant time to find or update weight of a specific token."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"images\\Index_Terms.JPG\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inverted Index: TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"images\\onefish-twofish.JPG\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indexing - How many passes through the data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### • TF and IDF for each token can be computed in one pass\n",
    "### • Cosine similarity also requires document lengths\n",
    "### • Need a second pass to compute document vector lengths\n",
    ">#### • Remember that the length of a document vector is the square-root of sum of the squares of the weights of its tokens.\n",
    ">#### • Remember the weight of a token is: TF * IDF\n",
    ">#### • Therefore, must wait until IDF’s are known (and therefore until all documents are indexed) before document lengths can be determined.\n",
    "### • Do a second pass over all documents: keep a list or hashtable with all document id’s, and for each document determine its length."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time Complexity of Indexing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### • Complexity of creating vector and indexing a document of n tokens is O(n).\n",
    ">#### • TF-IDF (do not divide TF by the maximum term frequency in a document).\n",
    "### • So indexing m such documents is O(m n).\n",
    "### • Computing token IDFs can be done during the same first pass\n",
    "### • Computing vector lengths is also O(m n).\n",
    "### • Complete process is O(m n), which is also the complexity of just reading in the corpus."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# #######################################\n",
    "# ######## Step 2: Indexing   ###########\n",
    "# #######################################"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# *************************\n",
    "# --BEGIN Create Postings Dictionary--\n",
    "# *************************"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Postings Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# create Postings dictionary\n",
    "\n",
    "# def getPostings(file_names, freq_word_PerDoc, perDocCorpClean)\n",
    "# return(postings)\n",
    "\n",
    "# postings = defaultdict(dict)\n",
    "\n",
    "postings = getPostings(file_names, freq_word_PerDoc, perDocCorpClean)\n",
    "postings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# //////////////////////////////////////////////////////////////////////\n",
    "# postings\n",
    "# \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n",
    "\n",
    "print('postings = \\n', postings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\nlen(positings) = ', len(postings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\nfreq_word_Corpus[0] = \\n', freq_word_Corpus[0])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# *************************\n",
    "# --END Postings Dictionary--\n",
    "# *************************"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create DF Dictionary"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# *************************\n",
    "# --BEGIN Create DF Dictionary--\n",
    "# *************************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# create DF dictionary\n",
    "\n",
    "# getDF(file_names, freq_word_Corpus, postings)\n",
    "# return(df)\n",
    "\n",
    "print('\\n-----CALCULATE DF-----\\n')\n",
    "\n",
    "# df = defaultdict(int)\n",
    "\n",
    "df = getDF(file_names, freq_word_Corpus, postings)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('\\ndf = ', df)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\nlen(df) = ', len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DFfirst20 = {k: df[k] for k in df.keys()[:20]}\n",
    "print('\\nDFfirst20 = ', DFfirst20)\n",
    "\n",
    "print('\\ntype(df) = ', type(df))\n",
    "print('\\nlen(df) = ', len(df))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# *************************\n",
    "# --END DF Dictionary--\n",
    "# *************************"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Inverted Index & docVecLen"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# *************************\n",
    "# --CREATE Inverted Index & docVecLen--\n",
    "# *************************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file_names, freq_word_Corpus\n",
    "print('\\nfile_names = ', file_names)\n",
    "print('\\freq_word_Corpus = ', freq_word_Corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create Inverted Index & docVecLen\n",
    "\n",
    "# getDocVecLen(file_names, freq_word_Corpus)\n",
    "# return(docVecLen)\n",
    "\n",
    "# docVecLen = defaultdict(float)\n",
    "\n",
    "docVecLen = getDocVecLen(file_names, freq_word_Corpus)\n",
    "docVecLen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# docVecLen values\n",
    "print('\\nOUTPUT of docVecLen values\\n')\n",
    "for docid in (range(len(file_names))):\n",
    "    print('docVecLen[docid] [', docid, '] = ', docVecLen[docid])\n",
    "print()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# *************************\n",
    "# --END Inverted Index & docVecLen--\n",
    "# *************************"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Retrival\n",
    "The output of your retrieval should be\n",
    "a list of (query id, document id) pairs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## • Input: Query and Inverted Index (from Step 2)\n",
    "## • Output: Similarity values between query and documents  \n",
    "  \n",
    "### • Tokens that are not in both the query and the document have no effect on the cosine similarity.\n",
    ">#### • Product of token weights is zero and does not contribute to the dot product.\n",
    "### • Usually the query is fairly short, and therefore its vector is extremely sparse.\n",
    "### • Use the inverted index (from Step 2) to find the limited set of documents that contain at least one of the query words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing the Query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### • Incrementally compute cosine similarity of each indexed document as query words are processed one by one.\n",
    "### • To accumulate a total score for each retrieved document:\n",
    ">#### • store retrieved documents in a hashtable, \n",
    ">#### • where the document id is the key and the partial accumulated score is the value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inverted Query Retrieval Efficiency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## • Assume that, on average, a query word appears in B documents:  \n",
    "  \n",
    "  <img src = \"images\\qWords-bDocs.JPG\">  \n",
    "  \n",
    "## • Then retrieval time is O(|Q|B), which is typically much better than:\n",
    ">#### • naïve retrieval that examines all |D| documents, O(|V||D|), \n",
    ">#### • because |Q| << |V| and B << |D|."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# #######################################\n",
    "# ######## Step 3: Retrival   ###########\n",
    "# #######################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Query from User"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# *************************\n",
    "# --BEGIN Get Query from User--\n",
    "# *************************"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# QUERY TEST TEXT TO ENTER:\n",
    "# Java ?& dog jUMp JUMPIMG 42 to also\n",
    "# Flow ?& shear hEAt heating 42 to well layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('QUERY TEST TEXT TO ENTER:')\n",
    "\n",
    "# print('Input for data-test folder:')\n",
    "# print('Java ?& dog jUMp JUMPING 42 to also')\n",
    "\n",
    "# print('Input for data folder:')\n",
    "# print('Flow ?& shear hEAt heating 42 to well layer')\n",
    "\n",
    "# print('\\n***---TEST DOC\\'s 0, 1, 2 & little 3 => [1]---***\\n')\n",
    "# print('the simple 42 ! situational PAST OF theoretical must least \\\n",
    "#      exactly accordingly specified')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# *************************\n",
    "# --\"queries.txt\" Input--\n",
    "# *************************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n\"queries.txt\" Input\\n')\n",
    "\n",
    "# print('Q1 = what investigations have been made of the wave system created \\\n",
    "#       by a static pressure distribution over a liquid surface .')\n",
    "print('Q2 = has anyone investigated the effect of shock generated \\\n",
    "     vorticity on heat transfer to a blunt body .')\n",
    "# print('Q3 = what is the heat transfer to a blunt body in the absence of \\\n",
    "#      vorticity .')\n",
    "# print('Q4 = what are the general effects on flow fields when the reynolds \\\n",
    "#      number is small .')\n",
    "# print('Q5 = find a calculation procedure applicable to all incompressible \\\n",
    "#      laminar boundary layer flow problems having good accuracy and \\\n",
    "#      reasonable computation time .')\n",
    "# print('Q6 = papers applicable to this problem (calculation procedures \\\n",
    "#       for laminar incompressible flow with arbitrary pressure \\\n",
    "#       gradient) .')\n",
    "# print('Q7 = has anyone investigated the shear buckling of stiffened \\\n",
    "#      plates .')\n",
    "# print('Q8 = papers on shear buckling of unstiffened rectangular plates \\\n",
    "#      under shear .')\n",
    "# print('Q9 = in practice, how close to reality are the assumptions that \\\n",
    "#      the flow in a hypersonic shock tube using nitrogen is non-viscous \\\n",
    "#      and in thermodynamic equilibrium .')\n",
    "# print('Q10 = what design factors can be used to control lift-drag ratios \\\n",
    "#      at mach numbers above 5 .')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query = []\n",
    "\n",
    "# queryTempFirst = \"Hi\"\n",
    "# queryTempLast = \"By\"\n",
    "# input = \"\"\n",
    "\n",
    "# input1 = input2 = input3 = input4 = input5 = \"\"\n",
    "# input6 = input7 = input8 = input9 = input10 = \"\"\n",
    "\n",
    "# query.append(queryTempFirst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('Pleae enter your queary:\\n')\n",
    "# input = (raw_input(\"Search query >> \"))\n",
    "\n",
    "# print('Input for data-test folder:')\n",
    "# input = \"Java ?& dog jUMp JUMPING 42 to also\"\n",
    "\n",
    "# print('Input for data folder:')\n",
    "# input = \"Flow ?& shear hEAt heating 42 to well layer\"\n",
    "\n",
    "# print('\\nTEST DOC [1] Input for data folder:')\n",
    "# input = \"the simple 42 ! situational PAST OF theoretical must least \\\n",
    "# exactly accordingly specified\""
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# *************************\n",
    "# --\"queries.txt\" Input--\n",
    "# *************************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\nInput for dir_path_queries folder: = ', dir_path_queries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('Input for <data-15> folder:')\n",
    "# input1 = \"what investigations have been made of the wave system created \\\n",
    "# by a static pressure distribution over a liquid surface .\"\n",
    "\n",
    "# print('Input for <data-15> folder:')\n",
    "# input2 = \"has anyone investigated the effect of shock generated \\\n",
    "# vorticity on heat transfer to a blunt body .\"\n",
    "\n",
    "# print('Input for <data-15> folder:')\n",
    "# input3 = \"what is the heat transfer to a blunt body in the absence of \\\n",
    "# vorticity .\"\n",
    "\n",
    "# print('Input for <data-15> folder:')\n",
    "# input4 = \"what are the general effects on flow fields when the reynolds \\\n",
    "# number is small .\"\n",
    "\n",
    "# print('Input for <data-15> folder:')\n",
    "# input5 = \"find a calculation procedure applicable to all incompressible \\\n",
    "# laminar boundary layer flow problems having good accuracy and reasonable \\\n",
    "# computation time .\"\n",
    "\n",
    "# print('Input for <data-15> folder:')\n",
    "# input6 = \"papers applicable to this problem (calculation procedures for \\\n",
    "# laminar incompressible flow with arbitrary pressure gradient) .\"\n",
    "\n",
    "# print('Input for <data-15> folder:')\n",
    "# input7 = \"has anyone investigated the shear buckling of stiffened plates .\"\n",
    "\n",
    "# print('Input for <data-15> folder:')\n",
    "# input8 = \"papers on shear buckling of unstiffened rectangular plates \\\n",
    "# under shear .\"\n",
    "\n",
    "# print('Input for <data-15> folder:')\n",
    "# input9 = \"in practice, how close to reality are the assumptions that the \\\n",
    "# flow in a hypersonic shock tube using nitrogen is non-viscous and in \\\n",
    "# thermodynamic equilibrium .\"\n",
    "\n",
    "# print('Input for <data-15> folder:')\n",
    "# input10 = \"what design factors can be used to control lift-drag ratios at \\\n",
    "# mach numbers above 5 .\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input = input2\n",
    "# print('input = input2')\n",
    "# print('input = ', input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query.append(input)\n",
    "# query.append(queryTempLast)\n",
    "# print('query = ', query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('query = ', query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# q = query\n",
    "# print('q = ', q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('\\nq = ', q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(' q[0] = ', q[0])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# *************************\n",
    "# --END Get Query from User--\n",
    "# *************************"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# *************************\n",
    "# ---BEGIN getQueries---\n",
    "# *************************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def getQueries(dir_path_queries)\n",
    "# return(queries_from_file)\n",
    "\n",
    "queries_from_file = getQueries(dir_path_queries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('queries_from_file = ', queries_from_file)\n",
    "print('type(queries_from_file) = ', type(queries_from_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('\\nqueries_from_file[:] = ', queries_from_file[:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\nqueries_from_file[0] = ', queries_from_file[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\nqueries_from_file[1:4] = ', queries_from_file[1:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print()\n",
    "for x in range(len(queries_from_file)):\n",
    "    print('queries_from_file[', x, '] = ', queries_from_file[x])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# *************************\n",
    "# ---END getQueries---\n",
    "# *************************"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## • Implement the preprocessing functions:  \n",
    ">### • For tokenization  \n",
    ">### • For stop word removal  \n",
    ">### • For stemming  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading Query as List"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# #####################################\n",
    "# ####### READING QUERY AS LIST #######\n",
    "# #####################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start Processing Q and Break Into Lines"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# *************************\n",
    "# ---BEGIN getQLines---\n",
    "# *************************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# start processing q and break into lines\n",
    "\n",
    "# def getQLines(q)\n",
    "# return(qReview, qDocnum, qTexts)\n",
    "\n",
    "# qReview, qDocnum, qTexts = getQLines(q)\n",
    "\n",
    "# print('\\nQREVIEW = \\n', qReview)\n",
    "# print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('type(qReview) = ', type(qReview))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# print('qReview = ', qReview)\n",
    "# print()\n",
    "# print('qDocnum = ', qDocnum)\n",
    "# print()\n",
    "# # print('titles = ', titles)\n",
    "# # print()\n",
    "# print('qTexts = ', qTexts)\n",
    "# print()\n",
    "# print('DONE ASSIGNING DOCNUM TITLES AND TEXTS')\n",
    "# print()\n",
    "# print('\\n-----END OF getQLines-----\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # print 1st 2 tokens in list review\n",
    "# print('qReview[:2] = ', qReview[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('qReview[0] = ', qReview[:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('qReview[0:3] = ', qReview[0:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('Number of tokens = len(qReview) = ', len(qReview))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('qTexts[0] = ', qTexts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# test_Qtext = qTexts[0]\n",
    "# test_Qtext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('test_Qtext = ', test_Qtext)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# *************************\n",
    "# ---END getQLines---\n",
    "# *************************"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Q Corpus"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# #####################################\n",
    "# ###### GET Q CORPUS #########\n",
    "# #####################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge TITLES & TEXTS into 1 String per Query"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# *************************\n",
    "# ---BEGIN getQCorp---\n",
    "# *************************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # merge TITLES & TEXTS into 1 STRING per Query\n",
    "\n",
    "# # def getQCorp(qTexts)\n",
    "# # return(qCorp)\n",
    "\n",
    "# qCorp = getQCorp(qTexts)\n",
    "\n",
    "# print('\\nqCorp = \\n', qCorp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('type(qCorp) = ', type(qCorp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# for i in range(len(qCorp)):\n",
    "#     print('\\nqCorp[', i, '] = ', qCorp[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# print('\\nqCorp[0:1] = \\n', qCorp[0:1])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# *************************\n",
    "# --END getQCorp--\n",
    "# *************************"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean Q Corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## • Output: Tokens to be added to the index  \n",
    ">### • No punctuation, no stop-words, stemmed"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# #######################################\n",
    "# ###### CLEAN Q CORPUS #########\n",
    "# #######################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For ea qCorp: tokenize, clean, stem, lem, stopwords, shortwords, etc."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# *************************\n",
    "# --BEGIN getQClean--\n",
    "# *************************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # for ea qCorp: tokenize, clean, stem, lem, stopwords,\\\n",
    "# # shortwords, etc.\n",
    "\n",
    "# # def getQClean(qCorp):\n",
    "# # return(qClean, qLen, fdistQ, fdistQLen, \n",
    "# #            freq_word_Q, freq_word_Qorpus)\n",
    "\n",
    "# qClean, qLen, fdistQ, fdistQLen, freq_word_Q , freq_word_Qorpus\\\n",
    "#     = getQClean(qCorp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('qClean[0:] = ', qClean[0:])\n",
    "# print('qClean = ', qClean)\n",
    "# print('fdistQ = ', fdistQ)\n",
    "# print('freq_word_Q = ', freq_word_Q)\n",
    "# print('qClean = ', qClean)\n",
    "\n",
    "# qClean = str(qClean[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('\\nqClean =\\n', qClean, '\\n')\n",
    "# x = 0\n",
    "# for x in range(len(qClean)):\n",
    "#     print('qClean[', x, '] = ', qClean[x])\n",
    "#     print('qLen[', x, '] = ', qLen[x])\n",
    "#     print('fdistQ[', x, '] = ', fdistQ[x])\n",
    "#     print('fdistQLen[', x, '] = ', fdistQLen[x])\n",
    "#     print('++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++')\n",
    "#     print('++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++')\n",
    "#     print('\\nfreq_word_Q[', x, '] = \\n', freq_word_Q[x])\n",
    "#     print('\\nfreq_word_Qorpus[', x, '] = \\n', freq_word_Qorpus[x])\n",
    "#     print()\n",
    "#     print('++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++')\n",
    "#     print('++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++')\n",
    "#     print()\n",
    "#     x += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# print('type(fdistQ) = ', type(fdistQ))\n",
    "# print('type(freq_word_Q) = ', type(freq_word_Q))\n",
    "# print('type(freq_word_Qorpus) = ', type(freq_word_Qorpus))\n",
    "# print('\\nfdistQ = ', fdistQ)\n",
    "# print('\\nfreq_word_Q = ', freq_word_Q)\n",
    "# print('\\nfreq_word_Qorpus = \\n', freq_word_Qorpus)\n",
    "# print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('\\nfreq_word_Q[:] = \\n', freq_word_Q[:], '\\n')\n",
    "# # print('\\nfreq_word_Q[0:-1] = \\n', freq_word_Q[0:-1], '\\n')\n",
    "\n",
    "# print('type(freq_word_Q) = ', type(freq_word_Q))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ####    freqQtest = []\n",
    "# ####    for i in (len(freq_word_Q)):\n",
    "# ####        freqQtest.append(freq_word_Q[i][0])\n",
    "\n",
    "# ####    print('\\nfreqQtest = ', freqQtest)\n",
    "# ####    print('\\ntype(freqQtest) = ', freqQtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print()\n",
    "# # print('type(qfdist) = ', type(qfdist))\n",
    "# print('type(qClean) = ', type(qClean))\n",
    "# print('type(fdistQ) = ', type(fdistQ))\n",
    "# print('type(freq_word_Q) = ', type(freq_word_Q))\n",
    "# print('type(freq_word_Qorpus) = ', type(freq_word_Qorpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#    print('\\nlen(qClean) = ', len(qClean))\n",
    "#    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# qClean0 = str(qClean[0])\n",
    "# qClean0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('\\nqClean0 =\\n', qClean0, '\\n')\n",
    "# print(type(qClean0))\n",
    "\n",
    "# # qSetClean = set((qClean0))\n",
    "\n",
    "# # qSetClean = set(qClean[:])\n",
    "# # qSetClean = set(freq_word_Q.items())\n",
    "\n",
    "# # for item in freq_word_Q:\n",
    "# #     qSetClean.add(item)\n",
    "\n",
    "# # print('\\nset of qSetClean = ', qClean0)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# *************************\n",
    "# --END getQClean--\n",
    "# *************************"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Tuples for Both the Query's Words & Freqs"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# *************************\n",
    "# --BEGIN Query Word & Freq Tuples--\n",
    "# *************************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # generate tuples for both the Query's Words & Freqs\n",
    "# #\n",
    "# # def getQTuples(freq_word_Q):\n",
    "# # return(q_tuple_words, q_tuple_freq_i)\n",
    "\n",
    "# q_tuple_words, q_tuple_freq_i = getQTuples(freq_word_Q)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# *************************\n",
    "# --TEST<<<BEGIN Query Word & Freq Tuples>>>TEST--\n",
    "# *************************"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "q_tuple_words = tuple([val for (key, val) in enumerate([val for elem in\n",
    "                       freq_word_Q for val in elem]) if key % 2 == 0])\n",
    "\n",
    "print(q_tuple_words)\n",
    "print(type(q_tuple_words))\n",
    "\n",
    "q_tuple_freq_i = tuple([val for (key, val) in enumerate([val for elem in\n",
    "                        freq_word_Q for val in elem]) if key % 2 != 0])\n",
    "\n",
    "print(q_tuple_freq_i)\n",
    "print(type(q_tuple_freq_i))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# *************************\n",
    "# --END TEST<<<BEGIN Query Word & Freq Tuples>>>TEST--\n",
    "# *************************"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# *************************\n",
    "# --END Query Word & Freq Tuples--\n",
    "# *************************"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate List of Relevant Documents"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# *************************\n",
    "# --BEGIN Document Retrieval--\n",
    "# *************************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # generate list of relevant documents\n",
    "\n",
    "# # def getRetDoc(postings, q_tuple_words)\n",
    "# # return(retDoc)\n",
    "\n",
    "\n",
    "# retDoc = getRetDoc(postings, q_tuple_words)\n",
    "# print('retDoc = ', retDoc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for docid in retDoc:\n",
    "#     print('retDoc[', docid, '] = ', docid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retDoc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# type(retDoc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for docid in retDoc:\n",
    "#     print('retDoc[', docid, '] = ', docid)\n",
    "\n",
    "# print('\\nretDoc = ', retDoc)\n",
    "# print('type(retDoc) = ', type(retDoc))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# *************************\n",
    "# --END Document Retrieval--\n",
    "# *************************"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate CosSim Scores b/t Q & Ea. Doc"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# *************************\n",
    "# --BEGIN getCosSimScoresList--\n",
    "# *************************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # calculate CosSim Scores b/t q & ea. doc\n",
    "\n",
    "# # def getCosSimScoresList(retDoc, q_tuple_words,\n",
    "# #                         q_tuple_freq_i, fdistCorpus):\n",
    "# # return(cosSimScoresList)\n",
    "\n",
    "# #     def getCosSim(docid, q_tuple_words, q_tuple_freq_i):\n",
    "# #     return(cosSim)\n",
    "\n",
    "# # cosSimScoresList = defaultdict(float)\n",
    "# # print('docid = ', docid)\n",
    "\n",
    "# print('retDoc = ', retDoc)\n",
    "# print('\\nq_tuple_words = ', q_tuple_words)\n",
    "# print('q_tuple_freq_i = ', q_tuple_freq_i)\n",
    "# print('\\nfdistCorpus = ', fdistCorpus)\n",
    "\n",
    "# cosSimScoresList = getCosSimScoresList(retDoc, q_tuple_words,\n",
    "#                                        q_tuple_freq_i, fdistCorpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # cosSimScoresList values\n",
    "\n",
    "# print('\\nOUTPUT of cosSimScoresList values\\n')\n",
    "# for docid in (range(len(cosSimScoresList))):\n",
    "#     print('cosSimScoresList[docid] [', docid, '] = ',\n",
    "#           cosSimScoresList[docid])\n",
    "# print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cosSimScoresList"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# *************************\n",
    "# --END getCosSimScoresList--\n",
    "# *************************"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# #############################################################################\n",
    "# Step 4: Ranking\n",
    "# #############################################################################"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# #######################################\n",
    "# ######## STEP 4: RANKING   ############\n",
    "# #######################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Ranking  \n",
    "2. For each of the ten queries in the queries.txt file, determine a ranked list of documents, in descending order of their similarity with the query.  \n",
    "\n",
    "Determine the average precision and recall for the ten queries, when you use:  \n",
    "• top 10 documents in the ranking  \n",
    "• top 50 documents in the ranking  \n",
    "• top 100 documents in the ranking  \n",
    "• top 500 documents in the ranking  \n",
    "  \n",
    "Note: A list of relevant documents for each query is provided to you, so that you can determine\n",
    "precision and recall."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### • Sort the hashtable including the retrieved documents based onthe value of cosine similarity\n",
    "### • Return the documents in descending order of their relevance  \n",
    "  \n",
    "## • Input: Similarity values between query and documents\n",
    "## • Output: Ranked list of documents in reversed order of their relevance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Term Weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### • Weights applied to both document terms and query terms\n",
    "### • Direct impact on the final ranking\n",
    ">#### • Direct impact on the results\n",
    ">#### • Direct impact on the quality of IR system"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rank List of Relevant Documents"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# *************************\n",
    "# --BEGIN getRankCosSimList--\n",
    "# *************************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # rank list of relevant documents\n",
    "\n",
    "# # def getRankCosSimList(cosSimScoresList)\n",
    "# # return(rankCosSimList)\n",
    "\n",
    "# # rankCosSimList = []\n",
    "\n",
    "\n",
    "# rankCosSimList = getRankCosSimList(cosSimScoresList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rankCosSimList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('\\nrankCosSimList = ', rankCosSimList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('\\ntype(rankCosSimList) = ', type(rankCosSimList))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#    '''\n",
    "#    rankCosSimList[ 0 ] =  (323, 0.36102506311675114)\n",
    "#    rankCosSimList[ 1 ] =  (322, 0.3515479432650447)\n",
    "#    rankCosSimList[ 2 ] =  (1394, 0.3512276970366803)\n",
    "#    rankCosSimList[ 3 ] =  (628, 0.3464697753459441)\n",
    "#    rankCosSimList[ 4 ] =  (179, 0.310596542107325)\n",
    "#\n",
    "#    VERSUS [(docid + 1) in cosSimScoresList]\n",
    "#\n",
    "#    rankCosSimList[ 0 ] =  (324, 0.36102506311675114)\n",
    "#    rankCosSimList[ 1 ] =  (323, 0.3515479432650447)\n",
    "#    rankCosSimList[ 2 ] =  (1395, 0.3512276970366803)\n",
    "#    rankCosSimList[ 3 ] =  (629, 0.3464697753459441)\n",
    "#    rankCosSimList[ 4 ] =  (180, 0.310596542107325)\n",
    "#    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##    print('\\nrankCosSimList = \\n', rankCosSimList)\n",
    "#    print('\\nlen(rankCosSimList) = ', len(rankCosSimList))\n",
    "#    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##    print()\n",
    "##    for y in range(len(rankCosSimList)):\n",
    "##        print('rankCosSimList[', y, '] = ', rankCosSimList[y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#    print()\n",
    "#    for y in xrange(5):\n",
    "#        print('rankCosSimList[', y, '] = ', rankCosSimList[y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##    print()\n",
    "##    test = []\n",
    "##    print('test = ', test)\n",
    "##    test.append(rankCosSimList[0])\n",
    "##    print('test.append(rankCosSimList[0]) = ', test)\n",
    "##    test.append(rankCosSimList[1])\n",
    "##    print('test.append(rankCosSimList[1]) = ', test)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# *************************\n",
    "# --END getRankCosSimList--\n",
    "# *************************"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# *************************\n",
    "# --BEGIN getRankListPerQ--\n",
    "# *************************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rank list of relevant documents per query\n",
    "\n",
    "# def getRankListPerQ(qNum, queries_from_file,\n",
    "#                    postings, fdistCorpus)\n",
    "# return(rankListPerQ)\n",
    "\n",
    "# rankListPerQ = []\n",
    "# output_qid_docid = []\n",
    "\n",
    "for qNum in range(len(queries_from_file)):\n",
    "    rankListPerQ = getRankListPerQ(qNum, queries_from_file,\n",
    "                                   postings, fdistCorpus)\n",
    "\n",
    "    for relvDocIdx in range(len(rankListPerQ)):\n",
    "        output_qid_docid.append((qNum + 1, rankListPerQ[relvDocIdx][0]))\n",
    "#         print('\\n\\noutput_qid_docid = ', output_qid_docid)\n",
    "\n",
    "    print('\\n\\n//////////////////////////////////////////////////////////')\n",
    "    print('output_qid_docid = ', output_qid_docid)\n",
    "    print('//////////////////////////////////////////////////////////\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n\\n//////////////////////////////////////////////////////////////')\n",
    "print('output_qid_docid = ', output_qid_docid)\n",
    "print('//////////////////////////////////////////////////////////////\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for n in range(len(output_qid_docid)):\n",
    "#     if output_qid_docid[n][0] == 1:\n",
    "#         print('output_qid_docid[', n, '] = ', output_qid_docid[n])\n",
    "#         print('docid = ', output_qid_docid[n][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n in xrange(5):\n",
    "    if output_qid_docid[n][0] == 1:\n",
    "        print('output_qid_docid[', n, '] = ', output_qid_docid[n])\n",
    "        print('docid = ', output_qid_docid[n][1])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# *************************\n",
    "# --END getRankListPerQ--\n",
    "# *************************"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# *************************\n",
    "# ---BEGIN sendToOutputFolder---\n",
    "# *************************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def sendToOutputFolder(dir_path_output, output_qid_docid)\n",
    "# return()\n",
    "\n",
    "sendToOutputFolder(dir_path_output, output_qid_docid)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# *************************\n",
    "# ---END sendToOutputFolder---\n",
    "# *************************"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# *************************\n",
    "# ---BEGIN getRelevance---\n",
    "# *************************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def getRelevance(dir_path_relevance):\n",
    "# return(relevance_from_file)\n",
    "\n",
    "relevance_from_file = getRelevance(dir_path_relevance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('\\nqueries_from_file = ', queries_from_file)\n",
    "print('\\ntype(relevance_from_file) = ', type(relevance_from_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('\\nqueries_from_file[:] = ', queries_from_file[:])\n",
    "print('\\nrelevance_from_file[0] = ', relevance_from_file[0])\n",
    "print('\\nrelevance_from_file[1:4] = ', relevance_from_file[1:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print()\n",
    "for x in xrange(5):\n",
    "    print('relevance_from_file[', x, '] = ', relevance_from_file[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print()\n",
    "print('relevance_from_file[]')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# *************************\n",
    "# ---END getRelevance---\n",
    "# *************************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\nrelevance_from_file[', 0, '] = ', relevance_from_file[0])\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for n in range(len(output_qid_docid)):\n",
    "#    if output_qid_docid[n][0] == 1:\n",
    "#        print('output_qid_docid[', n, '] = ', output_qid_docid[n])\n",
    "#        print('docid = ', output_qid_docid[n][1])\n",
    "\n",
    "for n in xrange(5):\n",
    "    if relevance_from_file[n][0] == 2:\n",
    "        print('relevance_from_file[', n, '] = ', relevance_from_file[n])\n",
    "        print('docid = ', relevance_from_file[n][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for n in range(len(relevance_from_file)):\n",
    "for r in xrange(5):\n",
    "#    if output_qid_docid[n][0] == 1:\n",
    "#    for tup in output_qid_docid[:]:\n",
    "    print('relevance_from_file[', r, '] = ', relevance_from_file[r])\n",
    "    print('qid = relevance_from_file[', r, '][0] = ',\n",
    "          relevance_from_file[r][0])\n",
    "    print('docid = relevance_from_file[', r, '][1] = ',\n",
    "          relevance_from_file[r][1], '\\n')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# *************************\n",
    "# ---BEGIN getQtyRelDocPerQ---\n",
    "# *************************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def getQtyRelDocPerQ(relevance_from_file):\n",
    "# return(qtyRelDocPerQ)\n",
    "\n",
    "qtyRelDocPerQ = getQtyRelDocPerQ(relevance_from_file)\n",
    "\n",
    "print('\\nqtyRelDocPerQ) = ', qtyRelDocPerQ)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# *************************\n",
    "# ---END getQtyRelDocPerQ---\n",
    "# *************************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Action"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# *************************\n",
    "# --BEGIN getAction--\n",
    "# *************************"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# generate list of relevant documents\n",
    "\n",
    "# def getRetDoc(postings, q_tuple_words)\n",
    "# return(retDoc)\n",
    "\n",
    "\n",
    "retDoc = getRetDoc(postings, q_tuple_words)\n",
    "print('retDoc = ', retDoc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# *************************\n",
    "# --END getAction--\n",
    "# *************************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### • Build an inverted index, with an entry for each word in the vocabulary\n",
    "### • Input: Tokens obtained from the preprocessing module\n",
    "### • Output: An inverted index for fast access\n",
    "### • Many data structures are appropriate for fast access\n",
    ">#### • We will use hashtables\n",
    "        * Store tokens in hashtable, with token string as key and weightas value.\n",
    "        * Table must fit in main memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### • Build an inverted index, with an entry for each word in the vocabulary\n",
    "### • Input: Tokens obtained from the preprocessing module\n",
    "### • Output: An inverted index for fast access\n",
    "### • Many data structures are appropriate for fast access\n",
    "<ul>\n",
    "    <li>We will use hashtables\n",
    "        <ul>\n",
    "            <li>Store tokens in hashtable, with token string as key and weight\n",
    "                as value.</li>\n",
    "            <li>Table must fit in main memory.</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### • Input: Documents that are read one by one from the collection\n",
    "### • Implement the preprocessing functions:  \n",
    ">#### • For tokenization  \n",
    ">#### • For stop word removal  \n",
    ">#### • For stemming  \n",
    "### • Output: Tokens to be added to the index  \n",
    ">#### • No punctuation, no stop-words, stemmed\n",
    "        - TWO\n",
    "        - THREE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### • Input: Documents that are read one by one from the collection\n",
    "### • Implement the preprocessing functions:  \n",
    ">#### • For tokenization  \n",
    ">#### • For stop word removal  \n",
    ">#### • For stemming  \n",
    "### • Output: Tokens to be added to the index  \n",
    ">#### • No punctuation, no stop-words, stemmed\n",
    "        - TWO\n",
    "        - THREE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ## Blockquoted header\n",
    ">\n",
    "> This is blockquoted text.\n",
    ">\n",
    "> This is a second paragraph within the blockquoted text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ## Blockquoted header\n",
    ">\n",
    "    > ### This is blockquoted text.\n",
    ">\n",
    "        > #### This is a second paragraph within the blockquoted text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ One\n",
    "+ Two\n",
    "+ Three\n",
    "    - Nested One\n",
    "    - Nested Two\n",
    "        * 3rd level one\n",
    "        * 3rd level two"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## + One\n",
    "## + Two\n",
    "## + Three\n",
    "    ### - Nested One\n",
    "    ### - Nested Two\n",
    "       #### * 3rd level one\n",
    "       #### * 3rd level two"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# --------------------DELETE----------------------"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "print('freq_word_Corpus[0] = ', freq_word_Corpus[0])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "freq_word_Corpus[0][0:1][0]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "freq_word_Corpus[0][0]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "word = 'layer'"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "word"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "if word in freq_word_Corpus[0][0]:\n",
    "    print('yes')\n",
    "else:\n",
    "    print('no')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "fdistCorpus[0]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "if word in fdistCorpus[0]:\n",
    "    print('yes')\n",
    "else:\n",
    "    print('no')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "termsPerDoc2 = []"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "type(termsPerDoc2)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "del(termsPerDoc2)\n",
    "type(termsPerDoc2)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "type(freq_word_PerDoc)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "freq_word_PerDoc[4]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "list(fdistPerDoc[4].items())"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "termsPerDoc2 = fdistPerDoc[4].items()\n",
    "print(termsPerDoc2)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "termsPerDoc2[4][4][0]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "termsPerDoc2[4][4][1]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "termsPerDoc2[4][-8][0]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "termsPerDoc2[4][-8][1]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "termsPerDoc2[4]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "termsPerDoc = fdistPerDoc[4].most_common()\n",
    "termsPerDoc\n",
    "print(termsPerDoc)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "termsPerDoc"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "list(fdistPerDoc[4].items())"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "fdistPerDoc"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "fdistPerDoc[4]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "fdistPerDoc[4]['slab']"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "fdistPerDoc[4][0][1]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "for k in fdistPerDoc[4].keys():\n",
    "    print(fdistPerDoc[4][k])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "for k,v in fdistPerDoc[4].items():\n",
    "    print(k,':',v)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "for k,v in fdistPerDoc[4].items():\n",
    "    print(k,':',v)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "fdistPerDoc[4].most_common()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "fdistPerDoc[4].most_common()[0]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "print('fdistPerDocLen[', 4, '] = ', fdistPerDocLen[4])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "print('fdistPerDoc[', 4, '] = ', fdistPerDoc[4][0:2][0:2])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "print('freq_word_PerDoc[', 4, '] = \\n', freq_word_PerDoc[4][0][2])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "print('perDocCorpClean[', 4, '] = ', perDocCorpClean[4][0])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "print('fdistPerDoc[', 4, '] = ', fdistPerDoc[4])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "print('freq_word_PerDoc[', 4, '] = \\n', freq_word_PerDoc[4][0])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "print('freq_word_PerDoc[', 4, '] = \\n', freq_word_PerDoc[4][1])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "print('freq_word_PerDoc[', 4, '] = \\n', freq_word_PerDoc[4][0][2])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# --------------------DELETE----------------------"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "print(freq_word_PerDoc[1][0:3])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# --------------------DELETE----------------------"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "fdistCorpus"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "fdistCorpus.items()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# --------------------DELETE----------------------"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "review2 = []\n",
    "\n",
    "dir_path2 = 'C:/Users/derekc/Dropbox/__cis833irtm/hw2/data'\n",
    "\n",
    "# get all files inside the directory\n",
    "files2 = os.listdir(dir_path2)\n",
    "    \n",
    "# tokenize the words based on white space, removes the punctuation\n",
    "for f2 in files2:\n",
    "    with open(dir_path2+'/'+os.path.basename(f2),'r') as ipfile2:\n",
    "        for i2 in ipfile2:\n",
    "            review2.append(i2)\n",
    "\n",
    "        # start processing the ipfile\n",
    "    #############################################################\n",
    "\n",
    "print(review2[:2])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# currently the data are in list ... convert to string. \n",
    "# even though there are many files, we will treat it as a single large document\n",
    "tokens2 = str(review2)\n",
    "\n",
    "# You can uncomment the following line if you would like to see to the output\n",
    "\n",
    "print(tokens2)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# print 1st 20 characters in list tokens\n",
    "print(tokens2[:100])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "#tokenization or word split\n",
    "tokens3 = word_tokenize(tokens2)\n",
    "\n",
    "# You can uncomment the following line if you would like to see to the output\n",
    "\n",
    "print(tokens3)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "tokens3[:20]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "####################################"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# The following is from array2dict"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "####################################"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "docnum = []\n",
    "titles = []\n",
    "docs = []\n",
    "docID = 0\n",
    "\n",
    "print('docnum = ', docnum)\n",
    "print('titles = ', titles)\n",
    "print('docID = ', docID)\n",
    "print()\n",
    "\n",
    "for i in range(len(review)):\n",
    "    docID = i+1\n",
    "    print('docID = ', docID)\n",
    "    print()\n",
    "    print('i = ', i)\n",
    "    docs.append(review[i])\n",
    "    print(docs[i])\n",
    "    print()\n",
    "    \n",
    "#     if review[i].isdigit() == True:\n",
    "#         docnum.append(int(review[i]))\n",
    "#         docID = int(review[i])\n",
    "#         print('docID = ', docID)\n",
    "#     elif review[i-1] == '<TITLE>':\n",
    "#         print(i)\n",
    "#         print('docID = ', docID)\n",
    "#         titles.append(review[i])\n",
    "#         print('reveiw[i] = ', review[i])\n",
    "#         print()\n",
    "        \n",
    "# #         j = i\n",
    "# #         while review[j] != '</TITLE>':\n",
    "# #             titles.append(review[j])\n",
    "    \n",
    "        \n",
    "print('docs = ', docs)    \n",
    "print('docnum = ', docnum)\n",
    "print('titles = ', titles)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "docs[0]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "docnum = []\n",
    "titles = []\n",
    "texts = []\n",
    "dict = {}\n",
    "j = 0\n",
    "\n",
    "if line.isdigit() == True:\n",
    "            line = int(line)\n",
    "            docnum.append(line)\n",
    "            review.append(line)\n",
    "            j += 1\n",
    "            \n",
    "        elif line == '<TITLE>':\n",
    "            while line != '</TITLE>':\n",
    "                titles.append(line)\n",
    "                review.append(line)\n",
    "        \n",
    "#         else:\n",
    "#             print('nothing')\n",
    "            \n",
    "                \n",
    "                \n",
    "                \n",
    "                \n",
    "        \n",
    "        #review = [x.strip() for x in ipfile()]\n",
    "        \n",
    "        #review.append(ipfile[line].rstrip('\\n'))\n",
    "\n",
    "    # start processing the ipfile\n",
    "#############################################################\n",
    "\n",
    "#print(review[0])\n",
    "print(review)\n",
    "print(docnum)\n",
    "print(titles)\n",
    "print(texts)\n",
    "print(j)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "####################################"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# The following is from roughdict & array2dict BeautifulSoup"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "####################################"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "print(str(review))|"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# currently the data are in list ... convert to string. \n",
    "# even though there are many files, we will treat it as a single large document\n",
    "tokens = str(review)\n",
    "\n",
    "# You can uncomment the following line if you would like to see to the output\n",
    "\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "#soup = BeautifulSoup(review).text\n",
    "#print(soup)\n",
    "\n",
    "soup = BeautifulSoup(tokens, \"lxml\")\n",
    "print(soup.get_text())"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "####################################"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# The following is from rough_dict & array2dict"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "####################################"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# currently the data are in list ... convert to string. \n",
    "# even though there are many files, we will treat it as a single large document\n",
    "tokens = str(review)\n",
    "\n",
    "# You can uncomment the following line if you would like to see to the output\n",
    "\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "tokens[:20]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "#import re\n",
    "\n",
    "def cleanhtml(raw_html):\n",
    "  cleanr = re.compile('<.*?>')\n",
    "  cleantext = re.sub(cleanr, '', raw_html)\n",
    "  return cleantext\n",
    "\n",
    "cleanhtml(tokens)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def remove_tags(text):\n",
    "    return ''.join(xml.etree.ElementTree.fromstring(text).itertext())\n",
    "\n",
    "remove_tags(tokens)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "import re\n",
    "def striphtml(data):\n",
    "    p = re.compile(r'<.*?>')\n",
    "    return p.sub('', data)\n",
    "\n",
    "#>>> striphtml('<a href=\"foo.com\" class=\"bar\">I Want This <b>text!</b></a>')\n",
    "#'I Want This text!'\n",
    "\n",
    "notags = striphtml(tokens)\n",
    "print(notags)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#import re\n",
    "sample = tokens\n",
    "          \n",
    "def striphtml(data):\n",
    "    p = re.compile(r'<.*?>')\n",
    "    return p.sub('', data)\n",
    "\n",
    "#>>> striphtml('<a href=\"foo.com\" class=\"bar\">I Want This <b>text!</b></a>')\n",
    "#'I Want This text!'\n",
    "\n",
    "notags = striphtml(sample)\n",
    "print(notags)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#import re\n",
    "sample = '<a href=\"foo.com\" class=\"bar\">I Want This <b>text!</b></a>'\n",
    "def striphtml(data):\n",
    "    p = re.compile(r'<.*?>')\n",
    "    return p.sub('', data)\n",
    "\n",
    "#>>> striphtml('<a href=\"foo.com\" class=\"bar\">I Want This <b>text!</b></a>')\n",
    "#'I Want This text!'\n",
    "\n",
    "notags = striphtml(sample)\n",
    "print(notags)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#import re\n",
    "sample = ('<DOC>\n",
    "<DOCNO>\n",
    "1\n",
    "</DOCNO>\n",
    "<TITLE>\n",
    "experimental investigation of the aerodynamics of a\n",
    "wing in a slipstream .\n",
    "</TITLE>\n",
    "<AUTHOR>\n",
    "brenckman,m.\n",
    "</AUTHOR>\n",
    "<BIBLIO>\n",
    "j. ae. scs. 25, 1958, 324.\n",
    "</BIBLIO>\n",
    "<TEXT>\n",
    "  an experimental study of a wing in a propeller slipstream was\n",
    "made in order to determine the spanwise distribution of the lift\n",
    "increase due to slipstream at different angles of attack of the wing\n",
    "and at different free stream to slipstream velocity ratios .  the\n",
    "results were intended in part as an evaluation basis for different\n",
    "theoretical treatments of this problem .\n",
    "  the comparative span loading curves, together with supporting\n",
    "evidence, showed that a substantial part of the lift increment\n",
    "produced by the slipstream was due to a /destalling/ or boundary-layer-control\n",
    "effect .  the integrated remaining lift increment,\n",
    "after subtracting this destalling lift, was found to agree\n",
    "well with a potential flow theory .\n",
    "  an empirical evaluation of the destalling effects was made for\n",
    "the specific configuration of the experiment .\n",
    "</TEXT>\n",
    "</DOC>')\n",
    "          \n",
    "def striphtml(data):\n",
    "    p = re.compile(r'<.*?>')\n",
    "    return p.sub('', data)\n",
    "\n",
    "#>>> striphtml('<a href=\"foo.com\" class=\"bar\">I Want This <b>text!</b></a>')\n",
    "#'I Want This text!'\n",
    "\n",
    "notags = striphtml(sample)\n",
    "print(notags)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#import re\n",
    "sample = ('<DOC> <DOCNO> 1 </DOCNO> <TITLE> experimental investigation of the aerodynamics of a wing in a slipstream . </TITLE> <AUTHOR> brenckman,m. </AUTHOR> <BIBLIO> j. ae. scs. 25, 1958, 324. </BIBLIO> <TEXT> an experimental study of a wing in a propeller slipstream was made in order to determine the spanwise distribution of the lift increase due to slipstream at different angles of attack of the wing and at different free stream to slipstream velocity ratios .  the results were intended in part as an evaluation basis for different theoretical treatments of this problem . the comparative span loading curves, together with supporting evidence, showed that a substantial part of the lift increment produced by the slipstream was due to a /destalling/ or boundary-layer-control effect .  the integrated remaining lift increment, after subtracting this destalling lift, was found to agree well with a potential flow theory . an empirical evaluation of the destalling effects was made for the specific configuration of the experiment . </TEXT> </DOC>')\n",
    "          \n",
    "def striphtml(data):\n",
    "    p = re.compile(r'<.*?>')\n",
    "    return p.sub('', data)\n",
    "\n",
    "#>>> striphtml('<a href=\"foo.com\" class=\"bar\">I Want This <b>text!</b></a>')\n",
    "#'I Want This text!'\n",
    "\n",
    "notags = striphtml(sample)\n",
    "print(notags)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Continuation of array2dict \"striphtml\" & \"cleanhtml\""
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# review = []\n",
    "# docnum = []\n",
    "# titles = []\n",
    "# texts = []\n",
    "# dict = {}\n",
    "# j = 0\n",
    "############################################################\n",
    "# path to cranfieldDocs directory\n",
    "\n",
    "test = []\n",
    "\n",
    "#dir_path = 'C:/Users/Derek Christensen/Dropbox/_cis833irtm/hw2/cranfieldDocs'\n",
    "dir_path = 'C:/Users/derekc/Dropbox/__cis833irtm/hw2/data-temp'\n",
    "\n",
    "# get all files inside the directory\n",
    "files = os.listdir(dir_path)\n",
    "    \n",
    "# tokenize the words based on white space, removes the punctuation\n",
    "for f in files:\n",
    "    with open(dir_path+'/'+os.path.basename(f), 'r') as ipfile:\n",
    "        for line in ipfile:\n",
    "            line = line.strip()\n",
    "            test.append(line)\n",
    "\n",
    "\n",
    "\n",
    "    #     for line in ipfile:\n",
    "    #         line = line.strip()\n",
    "    # #         print(line)\n",
    "\n",
    "    # #         print('hi')\n",
    "    #         if line.isdigit() == True:\n",
    "    #             line = int(line)\n",
    "    #             docnum.append(line)\n",
    "    #             review.append(line)\n",
    "    #             j += 1\n",
    "\n",
    "    #         elif line == '<TITLE>':\n",
    "    #             while line != '</TITLE>':\n",
    "    #                 titles.append(line)\n",
    "    #                 review.append(line)\n",
    "\n",
    "    #         else:\n",
    "    #             print('nothing')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            #review = [x.strip() for x in ipfile()]\n",
    "\n",
    "            #review.append(ipfile[line].rstrip('\\n'))\n",
    "\n",
    "    # start processing the ipfile\n",
    "#############################################################\n",
    "print(test)\n",
    "#print(review[0])\n",
    "# print(review)\n",
    "# print(docnum)\n",
    "# print(titles)\n",
    "# print(texts)\n",
    "# print(j)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# currently the data are in list ... convert to string. \n",
    "# even though there are many files, we will treat it as a single large document\n",
    "test = str(test)\n",
    "\n",
    "# You can uncomment the following line if you would like to see to the output\n",
    "\n",
    "#print(tokens)\n",
    "test"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#import re\n",
    "sample = test\n",
    "          \n",
    "def striphtml(data):\n",
    "    p = re.compile(r'<.*?>')\n",
    "    return p.sub('', data)\n",
    "\n",
    "#>>> striphtml('<a href=\"foo.com\" class=\"bar\">I Want This <b>text!</b></a>')\n",
    "#'I Want This text!'\n",
    "\n",
    "notags = striphtml(sample)\n",
    "print(notags)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# lowecases for content analytics ... we assume, for example, LOVE is sames love \n",
    "tokens = notags.lower()\n",
    "\n",
    "# You can uncomment the following line if you would like to see to the output\n",
    "\n",
    "#print(tokens)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# the dataset contains useless characters and numbers\n",
    "# Remove useless numbers and alphanumerical words\n",
    "# use regular expression ... a-zA-Z0-9 refers to all English characters (lowercase & uppercase) and numbers\n",
    "# ^a-zA-Z0-9 is opposite of a-zA-Z0-9\n",
    "tokens = re.sub(\"[^a-zA-Z0-9]\", \" \", tokens)\n",
    "\n",
    "#print(tokens)\n",
    "\n",
    "# You can uncomment the following line if you would like to see to the output\n",
    "\n",
    "tokens"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "print(tokens)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#tokenization or word split\n",
    "tokens = word_tokenize(tokens)\n",
    "\n",
    "# You can uncomment the following line if you would like to see to the output\n",
    "\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "print(len(tokens))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#import re\n",
    "sample = tokens\n",
    "          \n",
    "def striphtml(data):\n",
    "    p = re.compile(r'<.*?>')\n",
    "    return p.sub('', data)\n",
    "\n",
    "#>>> striphtml('<a href=\"foo.com\" class=\"bar\">I Want This <b>text!</b></a>')\n",
    "#'I Want This text!'\n",
    "\n",
    "notags = striphtml(sample)\n",
    "print(notags)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "####################################"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# The following is from HW2 Practice"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "####################################"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# print 1st 2 reviews\n",
    "print(review[:20])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "print(review[0:2])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "print(review[0][0])\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "review2 = []\n",
    "\n",
    "for i in review:\n",
    "    if review[i][0] != '<':\n",
    "        review2.append(i)\n",
    "        \n",
    "review2\n",
    "    "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "####################################"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# The following thru the End of Sec 3 is from HW1"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "####################################"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# currently the data are in list ... convert to string. \n",
    "# even though there are many files, we will treat it as a single large document\n",
    "tokens = str(review)\n",
    "\n",
    "# You can uncomment the following line if you would like to see to the output\n",
    "\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#tokenization or word split\n",
    "tokens = word_tokenize(tokens)\n",
    "\n",
    "# You can uncomment the following line if you would like to see to the output\n",
    "\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# print 1st 20 characters in list tokens\n",
    "print(tokens[:100])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# lowecases for content analytics ... we assume, for example, LOVE is sames love \n",
    "tokens = tokens.lower()\n",
    "\n",
    "# You can uncomment the following line if you would like to see to the output\n",
    "\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# print 1st 20 characters in list tokens\n",
    "print(tokens[:100])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# the dataset contains useless characters and numbers\n",
    "# Remove useless numbers and alphanumerical words\n",
    "# use regular expression ... a-zA-Z0-9 refers to all English characters (lowercase & uppercase) and numbers\n",
    "# ^a-zA-Z0-9 is opposite of a-zA-Z0-9\n",
    "tokens = re.sub(\"[^a-zA-Z0-9]\", \" \", tokens)\n",
    "\n",
    "#print(tokens)\n",
    "\n",
    "# You can uncomment the following line if you would like to see to the output\n",
    "\n",
    "tokens"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# print 1st 20 characters in list tokens\n",
    "print(tokens[:100])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "#tokenization or word split\n",
    "tokens = word_tokenize(tokens)\n",
    "\n",
    "# You can uncomment the following line if you would like to see to the output\n",
    "\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# print 1st 20 tokens in list tokens\n",
    "print(tokens[:20])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# 2. Determine the frequency of occurrence for all the words in the collection. Answer the following questions:<br>"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "## a. What is the total number of words in the collection?<br>"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "print(len(tokens))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "totwords = len(tokens)\n",
    "print(totwords)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "#compute frequency distribution for all the tokens in the text\n",
    "fdist = nltk.FreqDist(tokens)\n",
    "\n",
    "# You can uncomment the following line if you would like to see to the output\n",
    "\n",
    "#fdist"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "## b. What is the vocabulary size? (i.e., number of unique terms).<br>"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "print(len(fdist))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "## c. What are the top 20 words in the ranking? (i.e., the words with the highest frequencies).<br>"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "print(fdist.most_common(20))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "## d. From these top 20 words, which ones are stop-words?<br>"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Stop Words in the Top 20:\n",
    "- the\n",
    "- of\n",
    "- and\n",
    "- a\n",
    "- to\n",
    "- in\n",
    "- for\n",
    "- is\n",
    "- we\n",
    "- that\n",
    "- this\n",
    "- on\n",
    "- are\n",
    "- n\n",
    "- an\n",
    "- with\n",
    "- as\n",
    "- by\n",
    "\n",
    "## All of the Top 20 except the last two: data and based"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "## e. What is the minimum number of unique words accounting for 15% of the total number of words in the collection?<br>"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "totwords15 = totwords * 0.15\n",
    "print(totwords15)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# prepare the results of word frequency on corpus data as a list\n",
    "\n",
    "#freq_word_auto = []\n",
    "freq_word = []\n",
    "\n",
    "# two values or columns in fdist_a\n",
    "for k,v in fdist.items():\n",
    "    freq_word.append([k,v])\n",
    "\n",
    "#make it like an Excel worksheet\n",
    "wordlist = pd.DataFrame(freq_word)\n",
    "\n",
    "#pd.set_option('display.max_rows', 1000)\n",
    "pd.set_option('display.max_rows', 10)\n",
    "wordlist.sort_values(by=[1,0], ascending=[False, True])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "print(wordlist.loc[:,:])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "pd.set_option('display.max_rows', 10)\n",
    "sortwordlist = wordlist.sort_values(by=[1,0], ascending=[False, True])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "print(sortwordlist)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "print(wordlist[:][:])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "sortwordlist = sortwordlist.reset_index(drop=True)\n",
    "print(sortwordlist)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "print(sortwordlist[:5][:])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "sum = 0\n",
    "i = 0\n",
    "lastwordqty = 0\n",
    "\n",
    "print('Total number of words = ',totwords)\n",
    "print('Min number of unique words to account for 15% = ',totwords15)\n",
    "print()\n",
    "\n",
    "while sum < totwords15:\n",
    "    sum += sortwordlist.loc[i][1]\n",
    "    print(sortwordlist.loc[i][1])\n",
    "    print(sortwordlist.loc[i][0])\n",
    "    lastwordqty = sortwordlist.loc[i][1]\n",
    "    i += 1\n",
    "\n",
    "print()\n",
    "print(sum)\n",
    "print(i)\n",
    "print()\n",
    "\n",
    "diff = sum - lastwordqty\n",
    "print('Difference between Sum and the last word qty =',sum,'-',lastwordqty,'=',diff)\n",
    "print()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "sum = 0\n",
    "i = 0\n",
    "\n",
    "while sum < totwords15:\n",
    "    sum += sortwordlist.loc[i][1]\n",
    "    print(sortwordlist.loc[i][0], sortwordlist.loc[i][1])\n",
    "    i += 1\n",
    "\n",
    "print(sum)\n",
    "print(i)\n",
    "print('The minimum number of unique words to account for 15% fo the total number of words in the collection is', i,'.')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# 3. Integrate the Porter stemmer and a stopword eliminator into your code.\n",
    "Answer again questions a.-e. from the previous point."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Stemming/Lemmatization, Stopwords and Shortwords"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "source": [
    "#get stemming words or lemmas\n",
    "#wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "stemlemstop_tokens = (wordnet_lemmatizer.lemmatize(word) for word in tokens)\n",
    "stemlemstop_tokens\n",
    "\n",
    "# remove common words\n",
    "stoplist = stopwords.words('english')\n",
    "# if you want to remove additional words\n",
    "#more = set(['much','even','time','story','character','from','went','saw','movie','last','night','see','knew','films','film','one','one','would','also','seen','watch','dvd','get','bit','movies','two','three','whose'])\n",
    "#more = set(['the'])\n",
    "#stoplist = set(stoplist) | more\n",
    "stoplist = set(stoplist)\n",
    "\n",
    "stemlemstop_tokens = [[word for word in text if word not in stoplist] for text in tokens]\n",
    "\n",
    "# remove short words\n",
    "stemlemstop_tokens = [[word for word in tokens if len(word) >= 3 ] for tokens in stemlemstop_tokens]\n",
    "\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "print(tokens[:20])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "print(sortwordlist[:20])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "len(tokens)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "len(sortwordlist)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "#stemming\n",
    "stemmer = PorterStemmer()\n",
    "stemmed_tokens = [stemmer.stem(word) for word in tokens]\n",
    "print(stemmed_tokens[:20])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# remove common words\n",
    "stoplist = stopwords.words('english')\n",
    "# if you want to remove additional words\n",
    "#more = set(['much','even','time','story','character','from','went','saw','movie','last','night','see','knew','films','film','one','one','would','also','seen','watch','dvd','get','bit','movies','two','three','whose'])\n",
    "#more = set(['the'])\n",
    "#stoplist = set(stoplist) | more\n",
    "stoplist = set(stoplist)\n",
    "stemmed_stop_tokens = [word for word in stemmed_tokens if word not in stoplist]\n",
    "print(stemmed_stop_tokens[:20])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "print(len(stemmed_stop_tokens))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# Filter non-alphanumeric characters from tokens\n",
    "stemmed_stop_tokens = [word for word in stemmed_stop_tokens if word.isalpha()]\n",
    "print(len(stemmed_stop_tokens))\n",
    "print(stemmed_stop_tokens[:20])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "#remove short words\n",
    "stemmed_stop_tokens = [word for word in stemmed_stop_tokens if len(word) >= 3]\n",
    "print(len(stemmed_stop_tokens))\n",
    "print(stemmed_stop_tokens[:20])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "## a. What is the total number of words in the collection?<br>"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "print(len(stemmed_stop_tokens))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "fdist_stemmed_stop_tokens = nltk.FreqDist(stemmed_stop_tokens)\n",
    "\n",
    "# You can uncomment the following line if you would like to see to the output\n",
    "\n",
    "#fdist_stemmed_stop_tokens"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "## b. What is the vocabulary size? (i.e., number of unique terms).<br>"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "print(len(fdist_stemmed_stop_tokens))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "## c. What are the top 20 words in the ranking? (i.e., the words with the highest frequencies).<br>"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "fdist_stemmed_stop_tokens.most_common(20)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "## d. From these top 20 words, which ones are stop-words?<br>"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Stop Words in the Top 20:\n",
    "- thi (this)\n",
    "- use"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "## e. What is the minimum number of unique words accounting for 15% of the total number of words in the collection?<br>"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "totwords_stem = len(stemmed_stop_tokens)\n",
    "print(totwords_stem)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "totwords15_stem = totwords_stem * 0.15\n",
    "print(totwords15_stem)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# prepare the results of word frequency on corpus data as a list\n",
    "\n",
    "#freq_word_auto = []\n",
    "freq_word_stem = []\n",
    "\n",
    "# two values or columns in fdist_a\n",
    "for k,v in fdist_stemmed_stop_tokens.items():\n",
    "    freq_word_stem.append([k,v])\n",
    "\n",
    "#make it like an Excel worksheet\n",
    "wordlist_stem = pd.DataFrame(freq_word_stem)\n",
    "\n",
    "#pd.set_option('display.max_rows', 1000)\n",
    "pd.set_option('display.max_rows', 10)\n",
    "wordlist_stem.sort_values(by=[1,0], ascending=[False, True])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "print(wordlist_stem[:][:])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "pd.set_option('display.max_rows', 10)\n",
    "sortwordlist_stem = wordlist_stem.sort_values(by=[1,0], ascending=[False, True])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "print(sortwordlist_stem)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "sortwordlist_stem = sortwordlist_stem.reset_index(drop=True)\n",
    "print(sortwordlist_stem[:][:])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "sum_stem = 0\n",
    "j = 0\n",
    "lastwordqty_stem = 0\n",
    "\n",
    "print('Total number of words = ',totwords_stem)\n",
    "print('Min number of unique words to account for 15% = ',totwords15_stem)\n",
    "print()\n",
    "\n",
    "while sum_stem < totwords15_stem:\n",
    "    sum_stem += sortwordlist_stem.loc[j][1]\n",
    "    print(sortwordlist_stem.loc[j][1])\n",
    "    print(sortwordlist_stem.loc[j][0])\n",
    "    lastwordqty_stem = sortwordlist_stem.loc[j][1]\n",
    "    j += 1\n",
    "\n",
    "print()\n",
    "print(sum_stem)\n",
    "print(j)\n",
    "print()\n",
    "\n",
    "diff_stem = sum_stem - lastwordqty_stem\n",
    "print('Difference between Sum and the last word qty =',sum_stem,'-',lastwordqty_stem,'=',diff_stem)\n",
    "print()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "sum_stem = 0\n",
    "j = 0\n",
    "\n",
    "while sum_stem < totwords15_stem:\n",
    "    sum_stem += sortwordlist_stem.loc[j][1]\n",
    "    print(sortwordlist_stem.loc[j][0], sortwordlist_stem.loc[j][1])\n",
    "    j += 1\n",
    "\n",
    "print(sum_stem)\n",
    "print(j)\n",
    "print('The minimum number of unique words to account for 15% fo the total number of words in the collection is',j,'.')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Dictionary Practice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "idx = [1,2,3,4,5]\n",
    "vls = ['a','b','c','d','e']\n",
    "\n",
    "myzip = zip(idx,vls)\n",
    "print(idx)\n",
    "print(vls)\n",
    "print(myzip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dicttest = dict(myzip)\n",
    "print(dicttest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 4 in dicttest: print(dicttest[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dicttest[6] = 'f'\n",
    "print(dicttest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in dicttest:\n",
    "    print(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in dicttest:\n",
    "    print(dicttest[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in dicttest.iterkeys():\n",
    "    print(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in dicttest.iterkeys():\n",
    "    print(dicttest[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for val in dicttest.itervalues():\n",
    "    print(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in dicttest:\n",
    "    print(dicttest[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dicttest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dicttest[7] = 'g'\n",
    "print(dicttest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dicttest.update({1:'z'})\n",
    "print(dicttest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dicttest.update({2:'y', 8:'h'})\n",
    "print(dicttest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dicttest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Array Practice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(10)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x[-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.shape = (2,5)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x[1,3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x[1,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x[0][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x[0,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(10)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x[2:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x[:-7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x[1:7:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.arange(35).reshape(5,7)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y[1:5:2,::3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y[1:5:2,1::2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(10,1,-1)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x[np.array([3,3,1,8])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y =np.array(['a','be','cat','door'])\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hash Practice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D['a'] = 1\n",
    "D['b'] = 2\n",
    "D['c'] = 3\n",
    "D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D['b']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in D.keys():\n",
    "    print(D[k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k,v in D.items():\n",
    "    print(k,':',v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = ['d','e','f']\n",
    "values = [4,5,6]\n",
    "hash = {k:v for k, v in zip(keys, values)}\n",
    "hash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map(hash, [4, 5, 6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map(hash, [7,8,9,10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = ['at', 'be', 'cat', 'dog']\n",
    "docsat = ['d1', 'd2']\n",
    "docsbe = ['d2', 'd3', 'd5']\n",
    "docscat = ['d1', 'd5']\n",
    "\n",
    "freqval = [4, 2, 2, 4]\n",
    "freqat = [2, 1]\n",
    "freqbe = [3, 7, 4]\n",
    "freqcat = [5, 8]\n",
    "\n",
    "docfreqval ={d:f for d, f in zip(keys, freqval)}\n",
    "print(docfreqval)\n",
    "\n",
    "docfreqat ={d:f for d, f in zip(docsat, freqat)}\n",
    "print(docfreqat)\n",
    "\n",
    "docfreqbe ={d:f for d, f in zip(docsbe, freqbe)}\n",
    "print(docfreqbe)\n",
    "\n",
    "docfreqcat ={d:f for d, f in zip(docscat, freqcat)}\n",
    "print(docfreqcat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# numdocs = [2,3,3]\n",
    "\n",
    "# hash = {k:n:d for k, n, d in zip(keys, numdocs, )}\n",
    "\n",
    "numdocs = [2, 3]\n",
    "\n",
    "hash = {k:n for k, n in zip(keys, numdocs, )}\n",
    "print('hash = ', hash)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_files(dir, suffix):\n",
    "    \"\"\"\n",
    "    Returns all the files in a folder ending with suffix\n",
    "    :param dir:\n",
    "    :param suffix:\n",
    "    :return: the list of file names\n",
    "    \"\"\"\n",
    "    print('dir = ',dir)\n",
    "    print('suffix = ',suffix)\n",
    "    files = []\n",
    "    for file in os.listdir(dir):\n",
    "        if file.endswith(suffix):\n",
    "            files.append(file)\n",
    "        return(files)\n",
    "\n",
    "dir = 'C:/Users/Derek Christensen/Dropbox/_cis833irtm/hw2/data/'\n",
    "suffix = '.txt'\n",
    "files = []\n",
    "\n",
    "get_files(dir,suffix)\n",
    "print(files[:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# http://prooffreaderplus.blogspot.ca/2014/11/top-10-python-idioms-i-wished-id.html?m=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Python 3-style printing in Python 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enumerate a List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mylist = [\"It's\",'only','a','model.']\n",
    "\n",
    "for index, item in enumerate(mylist):\n",
    "    print(index, item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mylist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mylist[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mynumber = 3\n",
    "\n",
    "if 4 > mynumber > 2:\n",
    "    print(\"Chained comparison operators work! \\n\" * 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mycounter = Counter()\n",
    "for i in range(100):\n",
    "    random_number = randrange(10)\n",
    "    print(random_number)\n",
    "    mycounter[random_number] += 1\n",
    "for i in range(10):\n",
    "    print(i, mycounter[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mycounter[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum=0\n",
    "for i in range(10):\n",
    "    sum += mycounter[i]\n",
    "print(sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mycounter = Counter()\n",
    "\n",
    "# print(random_number)\n",
    "# print(mycounter[random_number])\n",
    "print('hi')\n",
    "for i in range(10):\n",
    "    print('i = ', i)\n",
    "    random_number = randrange(10)\n",
    "    print(random_number)\n",
    "    mycounter[random_number] += 1\n",
    "    print(mycounter[random_number])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dict Comprehensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_phrase = ['No','one','expects','the','Spanish','Inquisition']\n",
    "my_dict = {key:value for value, key in enumerate(my_phrase)}\n",
    "print(my_dict)\n",
    "rev_dict = {value:key for key, value in my_dict.items()}\n",
    "print(rev_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_phrase2 = ['Search','for','the','Holy','Grail']\n",
    "my_dict2 = {value:key for key, value in enumerate(my_phrase2)}\n",
    "print(my_dict2)\n",
    "rev_dict2 = {key:value for value, key in my_dict2.items()}\n",
    "print(rev_dict2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_phrase3 = ['grad','in','msor']\n",
    "my_dict3 = {key:value for key, value in enumerate(my_phrase3)}\n",
    "print(my_dict3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Executing Shell Commands with *subprocess*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import subprocess\n",
    "output = subprocess.check_output('dir', shell=True)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. *dict* *.get()* and *.iteritems()* Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_dict = {'name': 'Lancelot', 'quest': 'Holy Grail', 'favourite_color': 'blue'}\n",
    "\n",
    "print(my_dict.get('airspeed velocity of an unladen swallow', 'African or European?\\n'))\n",
    "\n",
    "for key, value in my_dict.iteritems():\n",
    "    print(key, value, sep=\": \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. *Tuple* unpacking for switching variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 'Spam'\n",
    "b = 'Eggs'\n",
    "\n",
    "print(a, b)\n",
    "\n",
    "a, b = b, a\n",
    "\n",
    "print(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9. Introspection tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_dict = {'That': 'an ex-parrot!'}\n",
    "    \n",
    "help(my_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10. PEP-8 compliant string chaining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_long_text = (\"We are no longer the knights who say Ni! \"\n",
    "                \"We are now the knights who say ekki-ekki-\"\n",
    "                \"ekki-p'tang-zoom-boing-z'nourrwringmm!\")\n",
    "print(my_long_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "https://www.ict.social/python/basics/multidimensional-lists-in-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\n",
    "\n",
    "# declaration and adding columns\n",
    "cinema = []\n",
    "for j in range(5):\n",
    "    column = []\n",
    "    for i in range(10):\n",
    "            column.append(i)\n",
    "    cinema.append(column)\n",
    "cinema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filling with data\n",
    "cinema[2][2] = 1 # center\n",
    "for i in range(1, 4): # fourth row\n",
    "    cinema[i][3] = 1\n",
    "for i in range(5): # the last row\n",
    "    cinema[i][4] = 1\n",
    "cinema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cinema[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = len(cinema)\n",
    "rows = 0\n",
    "if cols:\n",
    "    rows = len(cinema[0])\n",
    "for j in range(rows):\n",
    "    for i in range(cols):\n",
    "#         print(cinema[i][j])\n",
    "        print(cinema[i][j], end = \"\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\n",
    "\n",
    "# declaration and adding columns\n",
    "cinema = []\n",
    "for j in range(5):\n",
    "    row = []\n",
    "    for i in range(10):\n",
    "            row.append(0)\n",
    "    cinema.append(row)\n",
    "# filling with data\n",
    "cinema[2][2] = 1 # center\n",
    "for i in range(1, 4): # fourth row\n",
    "    cinema[i][3] = 1\n",
    "for i in range(5): # the last row\n",
    "    cinema[i][4] = 1\n",
    "\n",
    "rows = len(cinema)\n",
    "cols = 0\n",
    "if rows:\n",
    "    cols = len(cinema[0])\n",
    "for j in range(cols):\n",
    "    for i in range(rows):\n",
    "#         print(cinema[i][j])\n",
    "        print(cinema[i][j], end = \"\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cinema = []\n",
    "\n",
    "for j in range(5):\n",
    "        column = []\n",
    "        for i in range(10):\n",
    "                column.append(0)\n",
    "        cinema.append(column)\n",
    "cinema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('cinema = ', cinema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cinema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cinema[2][2] = 9 # center\n",
    "cinema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1, 4): # fourth row\n",
    "    cinema[i][3] = 7\n",
    "cinema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5): # the last row\n",
    "    cinema[i][4] = 1\n",
    "cinema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SETS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xlist = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### define your data as a list of flag values (True, False) mapped to flag names (single-character strings). You then transform this data definition into an inverted dictionary which maps flag names to flag values. This can be done quite succinctly with a nested list comprehension,"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def invert_dict(inverted_dict):\n",
    "    elements = inverted_dict.iteritems()\n",
    "    for flag_value, flag_names in elements:\n",
    "        print('flag_value = ', flag_value)\n",
    "        print('flag_names = ', flag_names)\n",
    "        for flag_name in flag_names:\n",
    "            print('flag_name = ', flag_name)\n",
    "            yield flag_name, flag_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def invert_dict(inverted_dict):\n",
    "    elements = inverted_dict.iteritems()\n",
    "    print('type(elements) = ', type(elements))\n",
    "    print('elements = ', elements)\n",
    "#     print('elements[:] = ', elements[:])\n",
    "#     print('elements[0] = ', elements[0])\n",
    "#     print('elements.items() = ', elements.items())\n",
    "#     print('elements.iteritems() = ', elements.iteritems())\n",
    "    print('elements.viewitems() = ', elements.viewitems())\n",
    "\n",
    "    for flag_value, flag_names in elements:\n",
    "        print('flag_value = ', flag_value)\n",
    "        print('flag_names = ', flag_names)\n",
    "        for flag_name in flag_names:\n",
    "            print('flag_name = ', flag_name)\n",
    "            yield flag_name, flag_value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flags = {True: [\"a\", \"b\", \"c\"], False: [\"d\", \"e\"]}\n",
    "flags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('flags = ', flags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flags = dict(invert_dict(flags))\n",
    "# >>> print flags\n",
    "# {'a': True, 'c': True, 'b': True, 'e': False, 'd': False}"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "type(elements) =  <type 'dictionary-itemiterator'>\n",
    "elements =  <dictionary-itemiterator object at 0x000000000EA3EC28>\n",
    "\n",
    "AttributeErrorTraceback (most recent call last)\n",
    "<ipython-input-88-66a576cde936> in <module>()\n",
    "----> 1 flags = dict(invert_dict(flags))\n",
    "      2 # >>> print flags\n",
    "      3 # {'a': True, 'c': True, 'b': True, 'e': False, 'd': False}\n",
    "\n",
    "<ipython-input-85-486cbc140d5f> in invert_dict(inverted_dict)\n",
    "      7 #     print('elements.items() = ', elements.items())\n",
    "      8 #     print('elements.iteritems() = ', elements.iteritems())\n",
    "----> 9     print('elements.viewitems() = ', elements.viewitems())\n",
    "     10 \n",
    "     11     for flag_value, flag_names in elements:\n",
    "\n",
    "AttributeError: 'dictionary-itemiterator' object has no attribute 'viewitems'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('flags = ', flags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# List Comprehension\n",
    "https://docs.python.org/2/tutorial/datastructures.html#list-comprehensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> # flatten a list using a listcomp with two 'for'\n",
    ">>> vec = [[1,2,3], [4,5,6], [7,8,9]]\n",
    ">>> [num for elem in vec for num in elem]\n",
    "[1, 2, 3, 4, 5, 6, 7, 8, 9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = [[1,2,3], [4,5,6], [7,8,9]]\n",
    "vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[num for elem in vec for num in elem]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[num for numlist in vec for num in numlist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = [[1,2,3], [4,5,6], [7,8,9]]\n",
    "vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[x for y in vec for x in y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[str(round(pi, i)) for i in range(1, 6)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practice Append list to List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = []\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.append([2, 156])\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = []\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b.append([1,11])\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = []\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c.append([3,55])\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b.append(c)\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.append(c)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.append(b)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Output Append"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tempOutput = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qNum = 1\n",
    "qNum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rankList = [[324, 0.3610], [323, 0.3515], [1395, 0.3512]]\n",
    "rankList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rankListTup = [(325, 0.3610), (322, 0.3515), (1394, 0.3512)]\n",
    "rankListTup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rankList[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('rankList[0] = ', rankList[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rankListTup[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('rankListTup[0] = ', rankListTup[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(rankListTup)):\n",
    "    tempOutput.append((qNum+1, rankListTup[i][0]))\n",
    "    print('tempOutput = ', tempOutput)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tempOutput[0][0] = qNum\n",
    "tempOutput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Max in a slice of an array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PA = []\n",
    "PA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PA = [0, 0, .35, .25, .20, .17, .22, .33, .08]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if PA[0] < max(PA[3:]):\n",
    "    PA[0] = max(PA[3:])\n",
    "PA[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if PA[1] < max(PA[3:]):\n",
    "    PA[1] = max(PA[3:])\n",
    "PA[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if PA[2] < max(PA[3:]):\n",
    "    PA[2] = max(PA[3:])\n",
    "PA[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if PA[3] < max(PA[4:]):\n",
    "    PA[3] = max(PA[4:])\n",
    "PA[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pal = len(PA)\n",
    "pal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if PA[4] < max(PA[5:]):\n",
    "    PA[4] = max(PA[5:])\n",
    "PA[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = 5\n",
    "if x < pal:\n",
    "    if PA[4] < max(PA[x:]):\n",
    "        PA[4] = max(PA[x:])\n",
    "PA[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PA[8] = .83"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = len(PA)-3\n",
    "print(x)\n",
    "if x < pal:\n",
    "    if PA[4] < max(PA[x:]):\n",
    "        PA[4] = max(PA[x:])\n",
    "PA[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SA = []\n",
    "SA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SA[0] = [11, 12, 13, 14]\n",
    "SA.append([11, 12, 13, 14])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SA[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SA.append([21, 22, 23, 24])\n",
    "SA[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SA[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SA.append([31, 32, 33, 34])\n",
    "SA[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SA[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SA[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SA[0][3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SA[1][3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "if PA[0] < max(PA[3:]):\n",
    "    PA[0] = max(PA[3:])\n",
    "PA[0]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "x = len(PA)-3\n",
    "print(x)\n",
    "if x < pal:\n",
    "    if PA[4] < max(PA[x:]):\n",
    "        PA[4] = max(PA[x:])\n",
    "PA[4]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
